[
["index.html", "An introduction to R and Radiant for a first course in Statistics Preface", " An introduction to R and Radiant for a first course in Statistics Sergio Venturini1 Department of Decision Sciences – Università Bocconisergio.venturini@unibocconi.it 2018-07-20 Preface In this manual we summarize the functionalities and the commands available in the R software that are presented in the course 30001 (Statistics) at Bocconi University. Because the syntax of the many R commands can sometimes be complex, to simplify the presentation and the study by students we decided to use a package (that is, an “extension” of R), called Radiant, which provides a graphical interface (via browser) for most of the techniques illustrated during the course. This approach will allow us to present more examples and spend more time in interpreting results rather than coding. Only in some situations, described in detail in this manual, we will directly use the R code, since in such contexts Radiant does not provide sufficiently adequate tools for our purposes. The manual is divided into five chapters plus an appendix. In the first chapter we describe how to install R and Radiant, some basic principles for the use of these software and the main ways to get information about the various commands (the so-called “help” of R). In the subsequent chapters we will show instead how to carry out the analyses presented in the course. More specifically, in the second chapter we will focus on descriptive techniques, with particular emphasis on data visualization. In the third chapter, the shorter of the manual, we will present the commands useful for carrying out probabilistic calculations. In chapter four it will be the turn of inferential techniques, namely estimation and hypotheses testing tools. We will dedicate the fifth chapter to the presentation of the commands for linear regression analysis. The final appendix contains further details for those interested in deepening the direct use of R, i.e. without the interface provided by Radiant, which can be seen as a skill also useful for future courses. We firmly believe that the learning of statistics cannot disregard the presentation of adequate calculation tools for the application of the various analyses to practical situations. This is the main motivation that has guided us in preparing the material for your statistics course and this manual. We hope to have succeeded in our intent. Enjoy the reading! \\(\\ddot \\smile\\) We acknowledge Gianna Monti and Piero Veronese for their precious collaboration.↩ "],
["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction In this chapter we present some preliminary concepts needed to introduce the commands described in the following chapters. A careful reading of this first chapter is of fundamental importance both to follow the course and to pass the exam. Therefore, we recommend you to setup your laptop from the very first lectures. We suggest you to closely follow the instructions given here for the installation of the various software. If you are not able to install all the necessary components, we invite you to request an operational support from the University ICT. For more specific aspects concerning the use of R, you can directly contact the instructors of your class. "],
["software-installation.html", "1.1 Software installation", " 1.1 Software installation In order to use the commands described in the rest of the manual, the following software must be downloaded and installed following the order presented here, that is the R application (http://www.r-project.org) the RStudio application (http://www.rstudio.com) the Radiant package (https://vnijs.github.io/radiant) Trivializing a little, we can say that the first component, R, represents the engine of our machine for statistical analysis, without which we can’t go anywhere. The second component, RStudio, represents the dashboard of the car with which we can control and adjust the power of the engine. Finally, the third component, Radiant, constitutes the outer body of the car, which gives the car a more pleasant and less technical appearance compared to its complicated internal mechanics. As we already mentioned in the introduction to this manual, it would not be strictly necessary to install all these components, as only the first one, R, would be enough. However, since R does not have a graphical interface, its use is not always intuitive. For this reason, we decided to simplify the learning process by introducing other software components that make the use of R as a computational engine more manageable. All the software we will be using are open-source, which means that they can be freely downloaded an unlimited number of times. In the next sections we report the instructions for installing the software separately for computers running Windows or Mac. The web pages we refer to in the next pages are not static but are modified and updated periodically. Therefore, the instructions given here could change even in the immediate future. In addition, the screenshots we will show in this manual have been created on a Mac computer set in English. Apart from some minor graphical differences, the information provided here applies exactly in the same way no matter what is your operation system or the language you are using. 1.1.1 Installation under Windows The instructions we provide here refer to the latest available version of Windows, that is Windows 102. 1.1.1.1 Downloading and installing R Open your browser and go to the Comprehensive R Archive Network (CRAN) home page at the following URL https://cran.r-project.org (see Figure 1.1). Figure 1.1: CRAN home page. Click on the link Download R for Windows and you will be redirected to the web page shown in Figure 1.2. Figure 1.2: CRAN page for downloading R for Windows. Then, click on the link install R for the first time and the web page provided in Figure 1.3 will be shown. Figure 1.3: CRAN page for downloading R for Windows in case it is the first you install it. Click on the link Download R 3.5.1 for Windows. This operation will download an executable file called R-3.5.1-win.exe3. Once downloaded, double click on the file you just downloaded. In Windows 10, a warning message will appear: “Windows protected your PC. Windows SmartScreen prevented an unrecognized app from starting. Running this app might put your PC at risk”. Click on More info and then on the Run anyway button. Windows provides this warning if your user account settings are the default settings and you are downloading software that does not come from the official Windows App Store. During the installation procedure we advise you to use the suggested default settings. Once installed, run R like any other software (for example, double-clicking on the desktop icon, or selecting it from the Windows Start menu). If you are using R on a 64-bit Windows computer (almost all current computers), you will find that both 64 and 32-bit versions have been installed. You can use one or the other, but we suggest using the 64-bit version (you could also delete the 32-bit icon from your desktop). 1.1.1.2 Downloading and installing RStudio RStudio is a type of application that programmers call integrated development environment (IDE), that is a development environment for R. An IDE is something different from a graphical user interface (graphical user interface, GUI), which instead provides a tool for interacting with an application and displaying its outputs. As we said, it would not be strictly necessary to go through the installation of RStudio, but this makes it easier to use the tools we will discuss later. Open the browser and go to the RStudio home page by connecting to https://www.rstudio.com (see Figure 1.4). Figure 1.4: RStudio home page. Click on Download RStudio in the banner you see at top of the page being then redirected to the page you see in Figure 1.5 Figure 1.5: Page for downloading RStudio. Click on the button labeled RStudio Desktop Open Source License, which is the free version (see Figure 1.6) Figure 1.6: Page for downloading RStudio. Finally, click on the RStudio 1.1.453 - Windows Vista/7/8/10 link, with which you will download an executable file called RStudio-1.1.453.exe4. Once downloaded, double click on the file and run the installation procedure. Once installed, run RStudio like any other software (for example, double-clicking on the desktop icon, or selecting it from the Windows Start menu). 1.1.1.3 Installing Radiant The R functions are organized in packages that refer to specific types of analysis. R comes with a basic set of packages that allow us to perform the most common statistical analyses. However, the basic functionality of R can be extended by installing additional packages (so-called contributed packages). Radiant is one of these packages, which provides a browser-based GUI for a good number of R functions. The Radiant web page is located at https://vnijs.github.io/radiant (see Figure 1.7), where you can find all the information on the installation and use of the package. In this manual we will present only the features of Radiant (as well as of R and RStudio) that are relevant to our course. Figure 1.7: Home page of the Radiant package. To install Radiant you need to start RStudio and in the window called Console (that is, in the window placed on the left side of the main application windows, see Figure 1.14) you need to type the following command5: &gt; install.packages(&quot;radiant&quot;, repos = &quot;https://radiant-rstats.github.io/minicran/&quot;, type = &#39;binary&#39;) This command will download and install not only Radiant, but also a number of other packages from which Radiant depends. At the end of the installation, you can start Radiant by executing the following command in the Console window &gt; radiant::radiant() or alternatively by choosing the command Start radiant (browser) which is available by clicking on the Addins button in the RStudio toolbar. Once started, Radiant will open your default browser and show the screen shown in Figure @ref(fig:Radiant_start). To stop the working session in Radiant simply choose Stop from the menu with the on/off symbol in the top menu bar. Figure 1.8: Starting screen of the Radiant package. You can find a series of video tutorial on how to install and use Radiant at web page https://radiant-rstats.github.io/docs/tutorials.html. 1.1.2 Installation under Mac 1.1.2.1 Downloading and installing R Open your browser and go to the Comprehensive R Archive Network (CRAN) home page located at https://cran.r-project.org (see Figure 1.1). Click on the Download R for (Mac) OS X link and you will be redirected to the web page shown in Figure 1.9. Figure 1.9: CRAN page for downloading R for Mac. Click on the link R-3.5.1.pkg6. This operation will download an executable file called R-3.5.1.pkg. Once downloaded, double click on the file and run the installation procedure. During the installation we advise you to use the suggested default settings. Once installed, run R like any other software (typically double-clicking its icon in the Applications folder). 1.1.2.2 Downloading and installing RStudio RStudio is a type of application that programmers call integrated development environment (IDE), which is a development environment for R. An IDE is something different from a graphical user interface (graphical user interface, GUI), which provides a tool for interacting with an application and displaying its outputs. As we said, it would not be strictly necessary to go through the installation of RStudio, but this makes it easier to use the tools we will discuss later. Open the browser and go to the RStudio home page by connecting to https://www.rstudio.com (see Figure 1.4). Click on Download RStudio in the initial banner and move to the page shown in Figure 1.5 Then click on the download button labeled RStudio Desktop Open Source License, which is the free version (see Figure 1.6)) Finally, click on the RStudio 1.1.453 - Mac OS X 10.6+ (64-bit) link, with which you will download an executable file called RStudio-1.1.453.dmg7. Once downloaded, double click on the file you just downloaded and run the installation procedure. Once installed, run RStudio like any other software, i.e. double-clicking on the icon in the Applications folder. 1.1.2.3 Installing Radiant The R functions are organized in packages that refer to specific types of analysis. R comes with a basic set of packages that allow us to perform the most common statistical analyses. However, the basic functionality of R can be extended by installing additional packages (so-called contributed packages). Radiant is one of these packages, which provides a browser-based GUI for a large number of R functions. The Radiant web page is located at https://vnijs.github.io/radiant (see Figure 1.7), where you can find all the information on the installation and use of the package. In this manual we will present only the features of Radiant (as well as of R and RStudio) that are relevant to our course. To install Radiant you need to start RStudio and in the window called Console (that is, in the window placed on the left side of the main application windows, see Figure 1.14) you need to type the following command8: &gt; install.packages(&quot;radiant&quot;, repos = &quot;https://radiant-rstats.github.io/minicran/&quot;, type = &#39;binary&#39;) This command will download and install not only Radiant, but also a number of other packages from which Radiant depends. At the end of the installation, you can start Radiant by executing the following command in the Console window &gt; radiant::radiant() or alternatively by choosing the command Start radiant (browser) which is available by clicking on the Addins button in the RStudio toolbar. Once started, Radiant will open your default browser and show the screen shown in Figure @ref(fig:Radiant_start). To stop the working session in Radiant simply choose Stop from the menu with the on/off symbol in the top menu bar. You can find a series of video tutorial on how to install and use Radiant at web page https://radiant-rstats.github.io/docs/tutorials.html. It is very likely that the same instructions will work also with older versions of Windows, but we are not able to guarantee it at 100%.↩ For the duration of the course we suggest to use the R version number 3.5.1. Therefore, even if later versions are available, we invite you to always install this version (previous versions of R can be retrieved by clicking on the Previous releases link also visible in Figure 1.3).↩ We remind you again that the version that you will find when you connect to this page may be different from the one shown here.↩ If you are planning to copy and paste the code in the console, remember to delete the initial symbol &gt;, which identifies the so-called command line, that is the point where you enter the commands you want to execute.↩ We reiterate that the version of R available at the time of the download may be later than 3.5.1. However, for the duration of the course we suggest using version 3.5.1. Therefore, even if later versions are available, we recommend that you always install this version (previous versions of R for Mac can be retrieved at https://cran.r-project.org/bin/macosx/el-capitan/base/).↩ The version you will find when you connect to this page may be different from the one shown here.↩ If you are planning to copy and paste the code in the console, remember to delete the initial symbol &gt;, which identifies the so-called command line, that is the point where you enter the commands you want to execute.↩ "],
["the-r-approach-to-statistics.html", "1.2 The R approach to statistics", " 1.2 The R approach to statistics R is an open-source software that provides a scripting language for statistics and data science at 360°. To date it is considered the most complete and advanced tool for performing any analyses that data science and Big Data analytics require. For this reason, all the commercial software for Statistics available on the market such as SPSS, SAS, etc., offer today the possibility of interfacing with R to integrate its functionalities with those of R. R is a language for manipulating “objects”. In fact, it is often said that in R “everything is an object”. In a broad sense, objects are tools for organizing different types of information. We can think of objects as containers whose size and type depend on its content. R provides many tools to create, display and manipulate the objects we need for our analysis. To understand what we mean by all this, we suggest you to open RStudio and type the following code at the command prompt: &gt; x &lt;- 5 The &gt; symbol represents the command prompt and should not be copied or reproduced for executing the code. Once typed, the command is executed by pressing the ENTER key. The &lt;- symbol, which corresponds to the characters &lt; (less than) and - (minus sign), represents the assignment operator, that is the operator with which new objects are created9. In the previous example we created a new object, which we decided to call x, but which we could have called in any other way, which simply contains the number 5. If the syntax of the command is correct, R does not produce any message, as in the example above. This means that the command does not contain any syntax error. To display the contents of an object, simply write the name of the object and press ENTER: &gt; x [1] 5 To get information about the structure (that is the characteristics) of an object, but without completely visualizing its content, we can use the function str(), for example &gt; str(x) num 5 The output returned informs us that the x object contains a single number (num). The str() function can be used to get information about the structure of any object created in R. As a further example, we execute the following code10: &gt; txt &lt;- c(&quot;R&quot;, &quot;is&quot;, &quot;a&quot;, &quot;software&quot;, &quot;for&quot;, &quot;data&quot;, &quot;science&quot;) &gt; str(txt) chr [1:7] &quot;R&quot; &quot;is&quot; &quot;a&quot; &quot;software&quot; &quot;for&quot; &quot;data&quot; &quot;science&quot; In this case, we have created a new object called txt that contains a vector of 7 text strings (the term chr that appears at the beginning of the output line indicates that the content of the object is of this type). The output of the str() function informs us of these characteristics. c() is the function to use for creating a vector, that is an object with a single dimension that can contain elements of the same type, in this case text strings11. If you want to show the list of objects created so far, you can use the function ls() &gt; ls() [1] &quot;txt&quot; &quot;x&quot; Recall that R is case sensitive, that is, it distinguishes between uppercase and lowercase characters. Finally, if you create a new object using the name of an existing object, you will clearly lose the old object, whose content will be replaced with that of the new one, for example: &gt; y &lt;- c(3, 4, -1, 0) &gt; str(y) num [1:4] 3 4 -1 0 &gt; y &lt;- &quot;Enjoy Statistics!&quot; &gt; str(y) chr &quot;Enjoy Statistics!&quot; Instead of the &lt;- symbol it is possible to use the equals sign (=), but it is conventionally in R to use the first one.↩ The spaces before and after commas or operators are not strictly necessary, but they help to make the code more readable.↩ A text string is characterized by being included within quotes.↩ "],
["main-data-structures-available-in-r.html", "1.3 Main data structures available in R", " 1.3 Main data structures available in R As we already mentioned above, in R there are different types of objects and in this section we will give a brief description of those we will use in the rest of the manual. 1.3.1 Basic data types In R it is possible to create objects containing numeric, textual or logical data, which in R are referred to as num, chr and logical respectively12. We have already seen examples of numerical and textual data, while the logical type data are usually obtained as a result of the verification of one or more logical conditions. For example, after defining the vector \\(x = (10, -2, 0.3, -1, 5)\\), that is &gt; x &lt;- c(10, -2, 0.3, -1, 5) suppose we want to verify which of its elements are positive and which are not. To do this, we simply execute the following code, where the result is stored in the new z object: &gt; z &lt;- (x &gt; 0) &gt; z [1] TRUE FALSE TRUE FALSE TRUE &gt; str(z) logi [1:5] TRUE FALSE TRUE FALSE TRUE As we see, z contains the answer to our question, which is given in the form of a vector of TRUE and FALSE values depending on whether the various elements of x are positive or negative. 1.3.2 Vectors This is the most important data structure in R represented by a set of elements of the same type, i.e. either all numbers, text strings or logical values. We have already seen in previous examples that we can create a vector with the c() function, which allows us to concatenate multiple elements into a single object. Many R functions return vectors as output, so it is important to learn how to manipulate this type of objects. The length() function returns the size of a vector, which is the number of elements that compose it: &gt; length(z) [1] 5 To select one or more elements of a vector we must use the [] (square brackets) operator. For example, we can extract the third element of the x vector with the code &gt; x[3] [1] 0.3 where the number 3 enclosed in square brackets indicates the position of the element we want to extract. In the same way we can select any subset of elements using the c() function. For example, to extract elements with position 1, 4 and 5 from x we can execute the code &gt; w &lt;- x[c(1, 4, 5)] &gt; w [1] 10 -1 5 Notice how in this case we extracted a subset of elements, the result of which was immediately stored in a new object that we called w. In R it is quite common, in fact, to create new objects as a result of some operation on existing objects. We can indicate the elements to extract also through the verification of logical conditions. For example, the following code extracts only the even elements from x: &gt; x[(x %% 2) == 0] [1] 10 -2 Note that in this example we used two new operators, the %% operator which returns the remainder of the division by the number on the right of the operator (in this case, the rest of the division of each element in x by 2), and the logical operator ==, which instead checks if the left side is equal to the right one and returns the logical values TRUE or FALSE as appropriate13. We also take this opportunity to emphasize that R automatically applies the various operations to all elements of a vector. It is possible to assign names to the elements of a vector through the names() function. This allows us to refer to the elements of the vector through their names instead of their positions: &gt; x [1] 10.0 -2.0 0.3 -1.0 5.0 &gt; names(x) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;) &gt; x A B C D E 10.0 -2.0 0.3 -1.0 5.0 &gt; x[c(&quot;B&quot;, &quot;E&quot;, &quot;A&quot;)] B E A -2 5 10 The names() function can be used both to get the names of the elements of a vector, but also, as we did above, to assign the names to the elements themselves. We conclude this part on vectors by adding that when we work with real data sets it is very common that some of the data is missing. The symbol in R that indicates a missing data is NA, which stands for not available (it corresponds to an empty cell in Excel). For example, suppose that in a market research we have collected the following data on annual income (in thousands of euros) for a sample of 10 individuals: &gt; income &lt;- c(40, 55, 60, NA, 34, 89, NA, NA, 121, 73) Three of the individuals interviewed did not report their income and are therefore are reported as NA. To know which data are missing, we can use the function is.na(), which returns a logical vector with the same length as that to which it is applied: &gt; is.na(income) [1] FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE FALSE FALSE To count how many missing we have, we can use the sum() function, which in general allows us to calculate the sum of the elements of a vector. In this case sum() first converts the logical TRUE and FALSE values returned by is.na() to 1 and 0 respectively, and then calculates their sum: &gt; sum(is.na(income)) [1] 3 1.3.3 Matrices Other useful data structures in R are matrices, that is, sets of elements of the same type (numbers, text, logical values) arranged along two dimensions, rows and columns, rather than along a single dimension as for vectors. To create a matrix we use the function matrix(), which is more articulated than the functions we have used so far, because it requires different arguments (for a more detailed discussion on the characteristics of a function object and a description of the type of arguments that can be passed to a function, see Section 1.3.714). The first argument requested, called data, corresponds to the matrix elements reported indifferently by row or by column. The second and third arguments, labeled respectively as nrow and ncol, indicate the number of rows and columns in which the elements in data must be arranged. The fourth argument, called byrow, is used to indicate R if the matrix is to be filled by rows (TRUE) or by columns (FALSE). In the following example, we create a \\((3 \\times 2)\\) matrix by rows: &gt; A &lt;- matrix(data = c(4, 2, 0, 1, -3, 0.9), nrow = 3, ncol = 2, byrow = TRUE) &gt; A [,1] [,2] [1,] 4 2.0 [2,] 0 1.0 [3,] -3 0.9 &gt; str(A) num [1:3, 1:2] 4 0 -3 2 1 0.9 The dim() function returns a vector with the dimensions of the matrix. To extract a subset of elements from a matrix we can use the same approaches as for vectors, except that now in the square brackets we must indicate both the row and the column indices of the elements to be extracted. If you intend to extract all the rows or all the columns of a matrix, it is sufficient not to indicate the respective index elements. The following code shows two examples using the A matrix defined above15: &gt; A[2, 1] # element in row 2 and column 1 [1] 0 &gt; A[c(1, 3), 2] # elements in row 1 or 3 and column 2 [1] 2.0 0.9 &gt; A[, 1] # all elements in column 1 [1] 4 0 -3 Now try a little exercise (check the answer by executing the code): what are the elements of the A matrix selected by the following code? &gt; A[A[, 2] &gt;= 1, A[3, ] &lt; 0] It is also possible to attribute names to the elements of a matrix. Since now we have two dimensions (i.e. rows and columns), there are two functions, called rownames() and colnames(), to assign names to the rows and columns of the matrix respectively: &gt; rownames(A) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) &gt; colnames(A) &lt;- c(&quot;d&quot;, &quot;e&quot;) &gt; A d e a 4 2.0 b 0 1.0 c -3 0.9 For example, after assigning names to rows and columns, we can select the third row of the A matrix as &gt; A[&quot;c&quot;, ] d e -3.0 0.9 It is possible to merge elements of matrices and vectors alongside one of the two dimensions (rows or columns) through the functions rbind() (to tile the rows) and cbind() (to tile the columns): &gt; D &lt;- matrix(c(1, 0, 0, 1), nrow = 2, ncol = 2, byrow = TRUE) &gt; b &lt;- c(-3, 4, 0) &gt; &gt; rbind(A, D) d e a 4 2.0 b 0 1.0 c -3 0.9 1 0.0 0 1.0 &gt; cbind(A, b) d e b a 4 2.0 -3 b 0 1.0 4 c -3 0.9 0 1.3.4 Factor vectors A factor vector is a vector that contains only predefined values and is used to represent qualitative/categorical variables. The factor vectors are defined internally by R as integers with text labels “attached”. To create a factor vector, we use the factor() function. For example, suppose that in the hypothetical survey that we described in Section 1.3.2, in addition to the annual income we also collected the gender of the respondents, reported as “m” and “f”: &gt; gender &lt;- factor(c(&quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;)) &gt; gender [1] f f f m m f m f f m Levels: f m To know the levels of a factor vector, use the levels() function: &gt; levels(gender) [1] &quot;f&quot; &quot;m&quot; The factor vectors will prove to be of fundamental importance for the analysis of categorical data. 1.3.5 Data frames A data frame is the data structure typically used in R to contain a data set. They are not to be confused with matrices, since the latter require their elements to be all of the same type, while the data frames may contain a mixture of numerical, categorical or textual columns. We can create a new data frame through the data.frame() function, even if the most common way to create one is to import a previously saved data (see Section 1.8). As an example, the following code creates a data frame starting from the income and gender vectors we created above, which will become the first and second columns of the data frame respectively: &gt; dat &lt;- data.frame(income, gender) &gt; str(dat) &#39;data.frame&#39;: 10 obs. of 2 variables: $ income: num 40 55 60 NA 34 89 NA NA 121 73 $ gender: Factor w/ 2 levels &quot;f&quot;,&quot;m&quot;: 1 1 1 2 2 1 2 1 1 2 Note that the str() function returns a summary of the information contained in the columns of the data frame, indicating for each one the corresponding data type, i.e. num and Factor with 2 levels, “f” and “m”. As with matrices, the dim() function returns the dimensions of a data frame (number of observations and number of variables), as well as it works in the same way both the selection of a subset of data and the assignment of row and column names: &gt; dat[5, 1] [1] 34 &gt; dat[, 2] [1] f f f m m f m f f m Levels: f m &gt; dat[c(1, 3, 4, 7), ] income gender 1 40 f 3 60 f 4 NA m 7 NA m &gt; rownames(dat) [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &gt; colnames(dat) [1] &quot;income&quot; &quot;gender&quot; In addition, we can select a column of a data frame using the $ (dollar sign)16. For example, to calculate the average annual income for the individuals included in the data set we can execute the following code: &gt; mean(dat$income, na.rm = TRUE) [1] 67.42857 We point out that in the previous code we have specified two arguments, the first corresponds to the variable for which we want to calculate the mean, while the second argument (na.rm, optional) indicates that, before calculating the mean, we want to remove any missing data17. If the data set is too large and it is not easy to print the entire content in the Console, we can display it with the function View(). To combine different data frames together, it is advisable to use the data.frame() function instead of cbind(). The data frames to be combined must have the same number of rows. 1.3.6 Lists Lists are very general objects whose elements can be other objects of any kind, including other lists. We can create a list with the function list(): &gt; X &lt;- list(y, txt, A) &gt; X [[1]] [1] &quot;Enjoy Statistics!&quot; [[2]] [1] &quot;R&quot; &quot;is&quot; &quot;a&quot; &quot;software&quot; &quot;for&quot; &quot;data&quot; [7] &quot;science&quot; [[3]] d e a 4 2.0 b 0 1.0 c -3 0.9 &gt; str(X) List of 3 $ : chr &quot;Enjoy Statistics!&quot; $ : chr [1:7] &quot;R&quot; &quot;is&quot; &quot;a&quot; &quot;software&quot; ... $ : num [1:3, 1:2] 4 0 -3 2 1 0.9 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:3] &quot;a&quot; &quot;b&quot; &quot;c&quot; .. ..$ : chr [1:2] &quot;d&quot; &quot;e&quot; Lists can be considered as a generalization of vectors whose elements are not bound to be of the same type. To retrieve the number of elements included in a list we must use the function length(), while to access one of its elements it is necessary to use the operator [[]} (double square brackets)18: &gt; X[[1]] # first element of the list X [1] &quot;Enjoy Statistics!&quot; &gt; X[[3]] # third element of the list X d e a 4 2.0 b 0 1.0 c -3 0.9 &gt; X[[3]][3, 2] # elements in row 3 and column 2 for the third element of X [1] 0.9 Lists are very important in R, because they are often used to store and organize the results of an analysis, such as those returned by the lm() function, which will be discussed in Chapter 5. 1.3.7 Functions The last type of objects we consider are functions. As objects, functions can be manipulated like all other objects available in R. However, the manipulation of functions is an advanced programming topic that we will not discuss here. Here, we limit ourselves to describe those features of functions that will be useful to us in the rest of the manual. Each function in R is characterized by three elements: the body of the function, or the code it contains the environment in which the function is evaluated the list of arguments, that are the function inputs We skip the details on the first two elements, since we will not need them in the rest of the course, while we give some more information on the list of arguments. As we already mentioned, the task of each function is to take a list of arguments in input, use these inputs to perform a series of calculations/operations and then return an output. Some functions return a numerical output, others a graphical output, others do not return anything but produce secondary effects. For example, the rownames() and colnames() functions produce no visible results, but they modify the names of the rows and columns of a matrix or data frame. As we have already verified in some examples, the output of a function can be stored in a new object, so that it can be retrieved later without having to perform the calculations again. As we said, the most important function element for our purposes is the list of arguments. Except for a few cases, such as the str() function, virtually all functions require one or more arguments. In case it is necessary to indicate many arguments, these must be separated by a comma (see the example on the function matrix() in Section 1.3.3). We can retrieve the complete list of arguments of a function with args(). For example, for the var() function, which calculates the sample variance of a set of numeric data, we obtain &gt; args(var) function (x, y = NULL, na.rm = FALSE, use) NULL This example shows that the var() function includes 4 arguments, x, y, na.rm and use, on which for now we do not provide further details. Thus, each argument has a name (for example, the third argument of var() is called na.rm) and the arguments of a function are presented in a certain order (for var(), first comes x, then y, and so on). When we want to use a function that requires many arguments, we need to make R able to understand the values to use for each of them. To do this we have the following alternatives: provide the arguments indicating for each the respective name; in this case we do not have to worry about listing the arguments in the order in which they are shown by args(), but we can write them in any order provide arguments without indicating their name, but in this case R expects that they are provided according to the order reported by args() both the approaches, providing some arguments with their names and others by entering them with their correct position; this approach is the most used in practice Let’s now look at some examples through the calculation of the variance of the annual income of the 10 interviewees in the hypothetical survey described above: &gt; var(x = income, na.rm = TRUE) # approach 1. --&gt; correct [1] 907.619 &gt; var(na.rm = TRUE, x = income) # approach 1. --&gt; correct [1] 907.619 &gt; var(income, NULL, TRUE) # approach 2. --&gt; correct [1] 907.619 &gt; var(income, TRUE) # approach 2. --&gt; error! Error in var(income, TRUE): incompatible dimensions &gt; var(income, na.rm = TRUE) # approach 3. --&gt; correct [1] 907.619 &gt; var(TRUE, x = income) # approach 3. --&gt; error! Error in var(TRUE, x = income): incompatible dimensions We conclude this introduction to functions by noting that it is not mandatory to provide all the arguments of a function. For example, in the previous examples on var() we never mentioned the use argument. This stems from the fact that almost all functions have some mandatory arguments and others that are optional. For example, the x argument of the var() function is mandatory: if you tried to execute the code var(na.rm = TRUE), you would get an error. If we did not explicitly specify the values of the optional arguments, R would use a default default for them, which is the one indicated by args(). For example, var() has only the first mandatory argument, while the others are optional, with na.rm which takes default value equal to FALSE19. The optional arguments, therefore, should be explicitly indicated only when we want to assign them a value different from the default one. The output returned by a function can be an object of any type (vector, matrix, data frame, list, or another function), even if some functions, especially those that produce graphical output, do not directly return any object. R also distinguishes between integers (int) and real numbers (num), but this distinction has no implication from our point of view.↩ The == logical operator is not to be confused with the equals sign (=).↩ Here, we only anticipate that the arguments of a function are divided into mandatory and optional. The optional arguments, such as byrow in the example below, assume a default value if they are not explicitly specified. The default values for the optional arguments of a function can be retrieved from the help page of the function itself (see Section 1.4).↩ The # character allows us to insert a comment, which is a text used to describe a piece of code, but which will not be executed by R.↩ One way to refer directly to the income column without having to specify the data frame that contains it, consists in using the attach() function, which makes the columns of a data frame available as independent objects during the current working session (the detach() function allows us to go back in this process). However, attach() should be used with extreme caution because different data frames may contain variables with the same name. For this reason, we will not use it further in this manual.↩ Not specifying the na.rm argument in presence of missing data results in a NA. This is the expected way in R to make us aware that a variable contains missing data.↩ If we used the single square brackets, instead of extracting the contents of the selected list element directly we would still get a list but with a single element corresponding to the selected element. If this explanation does not convince you, try running the code str(X[1]).↩ The y argument of var() takes the default value NULL, which in R indicates an empty object containing nothing.↩ "],
["howtogethelp.html", "1.4 How to get help on R commands", " 1.4 How to get help on R commands Since there are thousands of commands in R, each with its own list of arguments, it is practically impossible to memorize all of them. Every command in R has a very detailed documentation. There are two ways to get information about a command. If you know the name of the command, just type the question mark (?) followed by the name of the function itself (try for example with ?var). If you are working in RStudio20, this will show the help of the var() function in the tab called Help (see Figure 1.10). Figure 1.10: Help page of the var() function as it is shown in RStudio. All the help pages in R have the same structure and report the following information: in the Description section we find a brief description of the function in the Usage section we find the complete syntax of the function the Arguments section lists all the mandatory and optional arguments of the function with an indication of the default values of the latter the Details section shows the technical details of the calculations performed by the function the Value section describes the output returned by the function in the Note section we find further notes about the function the References section reports some bibliographical references the See Also section indicates other functions related to the one you are reading about in the Examples section we find some examples on the use of the function The second situation in which we can find ourselves is hat where we do not know the name of the command we need. In this case we can type a double question mark (??) followed by one or more keywords21. The proposed output consists of a list of help pages where the provided keywords are mentioned. As an example, try running the ??median code. If you are working directly in R, the help pages will be shown in the browser if you are using a Windows computer, or in a new window called R Help if use a Mac computer.↩ In case we indicate more than one keyword, you must include them within quotes.↩ "],
["packages.html", "1.5 R packages", " 1.5 R packages R comes with a very large set of functions organized in packages. In addition, more features can be added to R by installing additional packages. These packages, developed by the R user community, are all available for free. In order to use the functions contained in a package, you first need to install the package and then load it into the R memory. The first operation, the installation, must be performed only once (or every time you reinstall R on your computer or on a new computer), while the second one, the loading, must be performed every time you start R (or RStudio). There are several ways to install a package22, but here we present the simplest23: in RStudio choose the command Tools \\(\\rightarrow\\) Install Packages… and the window shown in Figure 1.11 will appear. Figure 1.11: Package installation in RStudio. In the Packages box, type the name of the package you want to install and check that the Install dependencies flag is selected. For example, Figure 1.12 shows the procedure for installing the Hmisc package, which contains a number of functions to perform calculations that would otherwise take a long time using the basic R. Figure 1.12: Installation of the Hmisc package in RStudio. After clicking on Install, RStudio it will download and install the necessary files (see Figure 1.13) Figure 1.13: Installation of the Hmisc package in RStudio. Once you have installed a package, to load it you must select it in the list called Packages (see Figure 1.15) which shows the list of packages installed in your local library R24. Together with the selected package, the packages it depends on will also be loaded. Remember that to install a package you need to be connected to the Internet.↩ An alternative method for installing packages uses the install.packages(), which you have already used to install the Radiant package (see Section 1.1.1.3).↩ An alternative way to load an already installed package is to use the library() function, indicating the name of the package we are interested in using. If the package we want to load is not already installed, library() will generate an error.↩ "],
["the-rstudio-application.html", "1.6 The RStudio application", " 1.6 The RStudio application RStudio is an open-source IDE that simplifies the use of R by integrating its documentation and allowing analyses to be organized in individual projects. The features provided by RStudio are numerous and its potential emerges particularly if you are interested in developing new functions or packages. In this section we provide a minimal introduction that will allow us to work more easily with R. When we start it, RStudio looks like in Figure 1.14. Figure 1.14: RStudio starting screen. The left side of the RStudio main window is occupied by the R Console, where, as we already did many times so far, it is possible to type in the commands we intend to execute and in which the output produced by the commands is printed. The right side is divided into two further panels25. The upper right panel is occupied by the tabs called Environment and History26. The first one shows the list of objects created so far with a summary for each of them, while the second one contains the chronological list of the commands executed so far, with the possibility to execute them again or save them in a files to be reused later. The lower right panel instead contains the following tabs: Files, which shows the contents of the current directory, with the possibility to change it by clicking on the name of the current directory Plots, which will sequentially report all the graphs produced during the working session Packages, which lists the packages installed on your computer and those that are currently loaded in memory (indicated by the check-marks next to the package names) Help, which shows the function help Viewer, that we will not use, but which allows us to view other types of output that RStudio can produce, such as PDF or HTML documents As we already mentioned, it is possible to execute one command at a time by typing it in the Console. Clearly this is not a convenient way to work with R, because once you have closed RStudio, the analysis will be lost. In order to reuse the commands and perform the corresponding analysis again at a later time, it is advisable to insert the commands in a script, a text file, which we can save and use later. To create a new script, choose File \\(\\rightarrow\\) New File \\(\\rightarrow\\) R Script. This operation will open a new tab temporarily called Untitled1 (see Figure 1.15) Figure 1.15: New script opening in RStudio. In this window we can write the code that interests us, which can then be executed in whole or in part27, and can be saved in a file with extension .R by choosing File \\(\\rightarrow\\) Save As…. All windows in RStudio can be resized and moved.↩ The tab Connections that you see in the figure may not be visible on your computer. In this case, do not worry, because we will not need it.↩ Select the code you are interested in and click on the Run. Alternatively, after selecting the code, you can press the Ctrl+ENTER keys on Windows or cmd+ENTER on Mac computers.↩ "],
["the-radiant-package.html", "1.7 The Radiant package", " 1.7 The Radiant package Radiant is an R package that provides a GUI for many of the functions we will cover in the course. Once you have installed Radiant as described in Sections 1.1.1.3 and 1.1.2.3, we can start it by executing the command &gt; radiant::radiant() The Radiant startup page, shown in Figure 1.16, shows the screen where you can load, edit and save a data set. Figure 1.16: Starting screen of the Radiant package. When we start it, Radiant shows an example data set (called diamonds). Each time you load a data set, Radiant shows a small portion of it in the output part called Data preview. The Radiant screens, accessed through the menus shown on top (Data, Design, Basics, etc.), are structured as a series of controls on gray background on the left, which change according to the chosen menu, and with a series of tabs within the screen that allow us to perform the various analyses. For example, the starting screen, which you can return to at any time by clicking on the Data menu you find on top left, includes the Manage, View, Visualize, Pivot, Explore, Transform and Combine tabs. In the following chapters we will see how some of these menus work. "],
["file-loading.html", "1.8 How to load and save a file in R and Radiant", " 1.8 How to load and save a file in R and Radiant R has two main file formats to store data, that is files with extension .RData (sometimes abbreviated to .rda) and those with extension .rds. Without going into the technical differences, we need only observe that the former type (.RData) allows to simultaneously save an indefinite number of objects, while the .rds format can save a single object at a time. In this manual, we will only use files of type .RData. To load the contents of an .RData file in RStudio, you must choose File \\(\\rightarrow\\) Open File…, select the file to open and confirm your choice. Alternatively, you can click on the file name in the Files tab and confirm. One last possibility uses the load() function. For example, if we want to load the file Forbes94.RData, which contains some data on the ranking published by the Forbes magazine in 1994 on the 800 most paid CEOs in the United States, the code to execute is: &gt; load(&quot;Forbes94.RData&quot;) Note that the file name must be enclosed within quotes because it is a text string. If the file was in a directory other than the current one, you must first change the working directory28, or indicate the entire path within the load() function (we discourage you to follow the latter approach because it is more complex and error-prone). After loading the Forbes94.RData file, in the Environment tab of RStudio you will find the new loaded objects, in this case only the data frame called forbes94. Try to get information on the characteristics of this object with the str() and View() functions. If the file to be opened is not in the .RData format, then one of the utilities provided by RStudio must be used to import data from other formats. You can find these utilities in the File \\(\\rightarrow\\) Import Dataset menu. It is possible to import data in text format (i.e. .txt or .csv), in Excel format (i.e. .xls or .xlsx), in SPSS format (i.e. .sav), in SAS format (i.e. .sas) or in Stata format (i.e. .dta). Selecting one of these items will open a window through which you can choose some settings (such as choosing whether the file uses the dot or comma as a decimal separator or if the first line of the file contains the variable names) and see a preview of the file you are about to import. Figure 1.17 shows this window for importing the Forbes94.xlsx file. The outcome of this procedure will be a data frame that will contain the required data according to the chosen settings. Figure 1.17: Excel file import procedure in RStudio. As an exercise, try repeating the procedure by importing the text file Forbes94.csv. To save one or more objects, we can use the save() function, which requires to specify the objects to be saved by indicating their names within quotes, as well as the argument file, that provides the name of the file where the objects will be saved. The following code saves the x, txt and A objects previously created in the attempt.RData file within the current directory: &gt; save(&quot;x&quot;, &quot;txt&quot;, &quot;A&quot;, file = &quot;attempt.RData&quot;) In Radiant you can load and save files from the Data screen. To load a file, you must first select the file format in the Load data of type box and then click on the Browse button to select the file. Radiant allows us to load files only in .RData, .rds and .csv formats. If the selected .RData file contains more than one object, Radiant will throw an error. To save the active data frame in Radiant, you must first choose the format in the Save data to type box (the available formats are the same as those for loading) and then press the Save button. This procedure will save the active data set in a file with the same name as the data set in the current working directory. To change the working directory, move to that directory in the Files tab, click on the icon with the gear (More) and choose Set As Working Directory.↩ "],
["descriptive-statistics.html", "Chapter 2 Descriptive statistics", " Chapter 2 Descriptive statistics In this chapter we describe some tools for performing descriptive analyses on a data set. As we discussed in the course, descriptive analyses typically consist of one or more tables, some summary indices and a series of graphs, which together allow to synthesize the salient characteristics of the empirical distribution of the data we are analysing. Here, we limit ourselves to the tools discussed during the course, but clearly both R and Radiant contain a series of other analyses that we will not cover in this manual. After illustrating some examples directly with R, we will present some tools available in Radiant to perform descriptive analyses, while we refer you to the appendix for those interested in the presentation of further R commands. In this chapter we will provide examples using data on the ranking published by the Forbes magazine in 1994 of the 800 most paid American CEOs in the United States, the data set we already presented in the previous chapter (see Section 1.8). "],
["descriptive-R.html", "2.1 First examples with R", " 2.1 First examples with R Before moving to Radiant, we would like to present some R commands, that is table(), pie() and boxplot(), which allow us to carry out simple descriptive analyses. The table() function allows us to create a frequency distribution for a single discrete qualitative/categorical or discrete numerical variable. As an example, let’s consider the variable MBA, that indicates the CEOs included in the data frame forbes94 that had an MBA title in 1994. The following code builds the frequency distribution of the variable MBA: &gt; table(forbes94$MBA) 0 1 589 211 Indeed, the output generated by table() is a vector whose elements are named with the observed categories of MBA. It is possible to graphically represent a frequency distribution for a categorical variable with a pie chart, available in R via the pie() function: &gt; pie(table(forbes94$MBA), main = &quot;Pie chart for MBA&quot;) Figure 2.1: Example of pie chart with R. The optional argument main, which takes the default value “” (that is, an empty text string), is used to add a title to the chart. As a last example of descriptive analysis carried out directly in R, we present the function boxplot(), which allows us to produce a box-plot for a single numerical variable. For example, the box-plot of the Salary variable is obtained as follows: &gt; boxplot(forbes94$Salary) Figure 2.2: Example of box-plot with R. Like all the functions that produce a graphical output, boxplot() also contains a long series of arguments, among which we find main, xlab and ylab, which respectively serve to add a title, the description of the horizontal axis and that of the vertical axis. A specific (logical) argument of boxplot() is horizontal, with which we can indicate whether to produce the box-plot horizontally (TRUE) or vertically (FALSE, default value): &gt; boxplot(forbes94$Salary, main = &quot;Box-plot of Salary&quot;, xlab = &quot;Salary (in $)&quot;, + horizontal = TRUE) Figure 2.3: Example of box-plot with R. "],
["univariate-descriptive-analysis.html", "2.2 Univariate descriptive analysis", " 2.2 Univariate descriptive analysis 2.2.1 A single categorical variable If we are interested in synthesizing the information related to a categorical variable, the tools we can use are the frequency distribution together with a graphical representation of it29. Before showing some examples, we remind you that in R categorical variables correspond to factor-type vectors, which we presented in Section 1.3.4. If the variable you intend to analyse is not coded as factor, you must first convert it into that format. Once the data set is loaded, we can do this in Radiant by moving to the Transform tab of the Data menu and following this procedure: select the variable in the list of variables on the left30 in the box labeled Transformation type, choose Change type in the box called Change variable type, choose As factor click on the green +Store button to confirm your choices In Figure 2.4 you can see the corresponding screenshot applied to the IndustryCode variable. Figure 2.4: Radiant page for the conversion of a numeric column in a factor vector. Be careful because, unless you first change the name of the data frame in the Store changes in box, pressing +Store will overwrite the original data frame31. To build the frequency distribution of a categorical variable in Radiant we can use the following procedure (see Figure 2.5): click on the Pivot tab in the Data menu click in the Categorical variables box and select the variable WideIndustry click on the green Create pivot table button to generate the table Figure 2.5: Radiant page for building a frequency distribution of a factor-type vector (that is, a categorical variable). The table will not be displayed completely unless you choose All in the Show … entries box above the table. You can also save the table in a CSV text file if you click on the downward-pointing arrow at the top right of the table. Finally, you can view the bar chart corresponding to the frequency distribution32 by clicking on the Show plot checkbox. If the number of variable levels is large (more than 15-20), we recommend to click on the Flip checkbox in the Plot type part to make the graph more readable. This option allows us to produce the chart horizontally. To report the relative frequencies in the table and in the graph instead of the absolute ones, click on the Normalize by box and choose Total. Please note that any changes to the table contents require updating the result, which can be done by clicking on the green button Update pivot table. 2.2.2 A single numerical variable The distribution of a numerical variable is represented by a table that shows the frequencies with which the different values were observed, if the variable is discrete, or the classes in which it is divided, if the variable is continuous. In both cases in Radiant you can still use the Pivot tab of the Data menu, but in the case of continuous variables you first need to divide the values into classes33. To graphically represent the distribution of a numerical variable, it is possible to use various types of graphs including the spike plot, the histogram and the box-plot. Radiant allows us to produce only the last two, while the box-plot can only be built for groups of observations corresponding to the categories of a categorical variable. In Radiant we can construct a histogram in the Visualize tab of the Data menu following this procedure: choose Distribution in the Plot-type box select the variable in the box called X-variable click on the green button Create plot The slider labeled Number of bins allows to choose the number of classes to use. Figure 2.6 shows the histogram with 10 classes of equal width for the variable Salary. Figure 2.6: Radiant page for building a histogram. Unfortunately Radiant does not allow to construct histograms with classes of different widths, but instead allows to generate histograms for data subgroups (i.e. it allows to graphically represent the conditional distributions of a variable with respect to the values taken by a second variable). For example, if we want to construct the histograms of conditionally upon the values of the MBA variable, which indicates whether each CEOs had an MBA title (value 1) in 1994 or not (value 0), in addition to the previous selections you need to choose also MBA in the box called Facet column and click on the green button Update plot. The corresponding graph is shown in Figure 2.7. Figure 2.7: Radiant page for building a histogram conditionally on the values of a second variable. As for box-plots, Radiant does not give the possibility to construct the box-plot for a single variable, but it allows to create box-plots for data subgroups, as we have just seen for histograms. The only difference is that to get the box-plots we have to choose Box-plot in the Plot-type box (see Figure 2.8). Finally, we note that, in presence of outliers, Radiant (and R too) builds the box-plot using a slightly different rule than the one presented in the course. For more details on the construction of the box-plots in Radiant, we invite you to see the help for this command, which can be accessed by clicking on the ? symbol at the bottom left of the screen. Figure 2.8: Radiant page for building a box-plot conditionally on the values of a second variable. The calculation of summary indices in Radiant can be done in the Explore tab, where you need to select the variables to analyse in the box named Numeric variable (s) and the list of indices to be calculated in the Apply function (s) box. In particular, clicking on the latter will show a list of available indices to choose from. As an example, we calculate the number of missing data (n_missing), the sample mean (mean), the sample standard deviation (sd), the sample quartiles (25% and 75%) and the sample coefficient of variation (cv) for the variables Salary, Bonus and Other, which indicate respectively the salary, bonuses and other compensation received by the CEOs in 1994. The result is shown in Figure 2.9. Figure 2.9: Radiant page for the calculation of summary indices. Through the Group by box it is possible to request the calculation of the indexes for data subgroups, in analogy to what has been seen for the histogram and the box-plot. In R and Radiant there is no function for the calculation of the mode, which can however be inferred from the frequency distribution and the corresponding graph.↩ You can apply this change simultaneously to a group of variables by selecting them all together in the list.↩ This operation will not overwrite the original .RData file, but only the local copy of the data frame loaded in the memory by Radiant.↩ Radiant does not give you the possibility to create a pie chart. In Section 2.1 we explained how to produce a pie chart directly in R.↩ You can do this by choosing the Transform tab in the Data menu, selecting Bin in the Transformation type box and indicating the number of classes (same width) that you intend to use. Be careful because this procedure requires that there are no missing data in the column to be recoded into classes. In presence of missing data, Radiant will show an error message.↩ "],
["bivariate-descriptive-analyses.html", "2.3 Bivariate descriptive analyses", " 2.3 Bivariate descriptive analyses The joint analysis of two variables makes it possible to verify whether there is any form of dependence between them. As for univariate analyses, the tools to be used depend on the type of variables involved in the analysis. 2.3.1 Two categorical variables In case we want to jointly analyse two categorical variables, the main tool to use is the two-way table. In Radiant we can construct a two-way table by moving to the Pivot tab, selecting the two variables to analyse in the Categorical variables box and clicking on the green Create pivot table button. An example for the variables MBA and WideIndustry is shown in Figure 2.10. Figure 2.10: Radiant page for building a two-way table. In Radiant it is possible to modify the table to obtain the relative frequencies or the corresponding conditional distributions by row or by column choosing respectively Total, Row or Column in the box labeled Normalize by. In Figure 2.11 we report the table with the conditional distributions of the MBA variable given the values of the WideIndustry variable. Figure 2.11: Radiant page for calculating a conditional distribution. A more effective way to analyse the association between a pair of categorical variables is through a bar chart that compares the conditional distributions of the column variable across the values of the row variable. In Radiant we can create a bar chart (side-by-side or stacked) by clicking on the checkbox named Show plot34. This operation produces a side-by-side bar chart, while clicking on Fill gives a stacked bar chart. In Figure 2.12 we report the stacked bar chart for the variable MBA against the variable WideIndustry, from which we can conclude that there is some association between the proportion of CEO who owned an MBA title in 1994 and the industry where the company operates. Figure 2.12: Radiant page for building a stacked bar chart. 2.3.2 Two numerical variables If we need to analyse two numerical variables together, the tools we can use are the scatter diagram, the sample covariance and the sample correlation index between the two variables. In Radiant you can perform these analyses with the Basics \\(\\rightarrow\\) Correlation menu. In the page that will appear, select the variables to analyse and click on the Show covariance matrix checkbox. The Plot tab of the same menu shows the scatter plot for each pairs of variables. In Figures 2.13 and 2.14 we report the Radiant output for the variables Age, Salary, Bonus and Other. Figure 2.13: Radiant page to calculate the sample covariances and correlations for a set of variables. Figure 2.14: Radiant page to create the scatter diagrams for a set of variables. To obtain a bar chart that can be interpreted as we discussed in the course, in Radiant it is necessary to exchange the order of the variables.↩ "],
["probability.html", "Chapter 3 Probability", " Chapter 3 Probability This chapter is dedicated to illustrate some tools for calculating probabilities using Radiant. In particular, we will present the commands available in the Basics \\(\\rightarrow\\) Probability calculator menu, with which you obtain the screenshot shown in Figure 3.1. Figure 3.1: Radiant page for probability calculations. This menu allows us to perform probabilistic calculations using one of the probability distributions listed in the box Distribution, of which we present here the details only for those discussed during the course, namely: the binomial distribution (and hence the Bernoulli distribution too) the normal (or Gaussian) distribution Student’s \\(t\\) distribution the \\(\\chi^2\\) (chi-square) distribution Fisher’s \\(F\\) distribution For each of these distributions, by clicking on Input type \\(\\rightarrow\\) Values, Radiant is able to calculate the probability that the corresponding random variable is in the range specified in the Lower bound and Upper bound boxes. Alternatively, by choosing Input type \\(\\rightarrow\\) Probabilities, it is possible to retrieve the values of the random variable corresponding to the probabilities provided in the Lower bound and Upper bound boxes. "],
["binomial-random-variables.html", "3.1 Binomial random variables", " 3.1 Binomial random variables To perform probabilistic calculations using a binomial random variable, select the Binomial entry in the Distribution box. The page you would see is shown in Figure 3.2. Figure 3.2: Radiant page for calculating probabilities related to a binomial random variable. The screenshot shows the values of the distribution parameters, i.e. the number of Bernoulli trials n and the probability of a single success p in each trial, and the corresponding graph of the probability function where the bars are colored according to the values shown in the Lower bound and Upper bound boxes. In the central part of the screen you can find the values of the mean and standard deviation and the probabilities corresponding to certain intervals provided again with the Lower bound and Upper bound boxes. Suppose we want to calculate the probability with which a binomial random variable with parameters \\(n = 8\\) and \\(p = 0.3\\) takes a value in the range \\([4, 7)\\) (note that the interval does not include the upper boundary value). In this case we need: enter the values of the parameters in the n and p boxes if not already selected, click on Input type \\(\\rightarrow\\) Values in the Lower bound and Upper bound boxes enter 4 and 7 respectively we also suggest to increase to 4 the number of decimal places shown (Decimals) The resulting screenshot is shown in Figure 3.3. Figure 3.3: Radiant page for calculating probabilities related to a binomial random variable. The probability we were looking for, that is \\(P(4 \\le X &lt; 7)\\) with the value 7 excluded, is not reported in the table, that instead provides \\(P(4 \\le X \\le 7)\\), where the value 7 is included instead. This issue is easily solved because the same table also reports the value of \\(P(X = 7)\\), which must then be subtracted from \\(P(4 \\le X \\le 7)\\) to get \\[\\begin{equation*} P(4 \\le X &lt; 7) = P(4 \\le X \\le 7) - P(X = 7) = 0.1940 - 0.0012 = 0.1928. \\end{equation*}\\] Equivalently, the same result can be achieved by directly entering the values 4 and 6 in the Lower bound and Upper bound boxes. It is understood that the same Radiant page can also be used for Bernoulli random variables by setting the number of tests n to 1. "],
["normal-or-gaussian-random-variables.html", "3.2 Normal (or Gaussian) random variables", " 3.2 Normal (or Gaussian) random variables Choosing Normal in the box Distribution, we get the page for calculating probabilities involving a normal random variable (see Figure 3.4), which actually represents an electronic and more general version of the standard normal distribution table presented in the course. Figure 3.4: Radiant page for calculating probabilities related to a normal random variable. If we are interested in calculating a probability for a range of values, we must click on Input type \\(\\rightarrow\\) Values, while if we are interested in the opposite situation, that is the calculation of the interval boundaries corresponding to a certain probability value, we must choose Input type \\(\\rightarrow\\) Probabilities. Let’s see an example for each of these cases. Suppose we want to calculate the probability that a random variable \\(X\\) distributed according to a normal with mean equal to 50 and variance equal to 64, or \\(X \\sim N(\\mu = 50, \\sigma^2 = 64)\\), takes value in the interval \\((45, 60)\\). In this case we must: enter the values of the parameters in the Mean and St. dev. boxes35 if not already selected, click on Input type \\(\\rightarrow\\) Values in the Lower bound and Upper bound boxes, enter 45 and 60 respectively we also suggest to increase to 4 the number of decimal places shown (Decimals) The resulting screenshot is shown in Figure 3.5. Figure 3.5: Radiant page for calculating probabilities related to a normal random variable. The value we were looking for is P(45 &lt; X &lt; 60) = 0.6284. Please note that if you leave one of the Lower bound or Upper bound boxes empty, this is interpreted by Radiant as an unbounded range on the left or right36. If, on the other hand, we want to derive the probability corresponding to a given range of values, we must select Input type \\(\\rightarrow\\) Probabilities. For example, continuing with the previous example, suppose we are interested in obtaining the value \\(x\\) of the random variable \\(X\\) that will not be exceeded with probability equal to 95%, i.e. the value \\(x\\) such that \\(P(X \\le x) = 0.95\\). In this case we must: check that the parameter values in the Mean and St. dev. boxes are the correct ones if not already selected, click on Input type \\(\\rightarrow\\) Probabilities in the box Upper bound enter the value 0.95 and either leave blank or enter the value 0 in the box Lower bound make sure you have indicated 4 decimal places in the box Decimals The resulting screenshot is shown in Figure 3.6. Figure 3.6: Radiant page for calculating probabilities related to a normal random variable. The value we were looking for is P(X &lt; 63.1588) = 0.95, that is \\(x = 63.1588\\). Warning, Radiant prompts you to enter the value of standard deviation, not the variance. In our case \\(\\sigma = \\sqrt{64} = 8\\).↩ In R, and therefore also in Radiant, an infinite value is referred to as Inf.↩ "],
["other-random-variables.html", "3.3 Other random variables", " 3.3 Other random variables Details for the other random variables presented in the course (ie \\(t\\), \\(\\chi^2\\) and \\(F\\)) are similar those discussed above for the normal distribution, with the exception that these distributions are defined by a different set of parameters. We report the corresponding Radiant screenshots in Figures 3.7, 3.8 and 3.9. Figure 3.7: Radiant page for calculating probabilities related to a \\(t\\) random variable. Figure 3.8: Radiant page for calculating probabilities related to a \\(\\chi^2\\) random variable. Figure 3.9: Radiant page for calculating probabilities related to an \\(F\\) random variable. "],
["inferential-statistics.html", "Chapter 4 Inferential statistics", " Chapter 4 Inferential statistics In this chapter we show how to apply the inferential analyses presented in the course. We start from the simplest case of a single mean for a normal population and then move on to more complex cases, including the goodness-of-fit test for a distribution with fully specified probabilities and the independence test in a two-way table. In this chapter we present the tools available in Radiant, while we refer to the appendix for an in-depth analysis of other R functions. "],
["inference-on-the-mean-of-a-normal-population.html", "4.1 Inference on the mean of a normal population", " 4.1 Inference on the mean of a normal population It is possible to make inference on an mean \\(\\mu\\) of a normal population assuming that the variance \\(\\sigma^2\\) of the population is known or not. In this section we deal only with the second case because it is the most frequent and relevant situation in practice. In addition, neither R nor Radiant provide commands to deal with the case where the variance is known. For the case of a single mean of a normal population, Radiant includes the Basics \\(\\rightarrow\\) Single mean menu. Let’s see an example using again the data contained in the data frame forbes94. In particular, we define a new variable that we call ROS, that is return on sales, calculated as the ratio (as a percentage) of profits with respect to sales revenues. This quantity expresses the company profitability in relation to the profitability of the stream of revenues. To create the new ROS variable in Radiant you must select the Transform tab in the Data menu, then select the Create entry in the Transformation type box, define the new variable by typing ROS = Profits/Sales*100 in the Create box, press ENTER and then click on the green +Store button to add the variable to the active data frame. The screenshot for the inference on the mean of a normal population, of which we show in Figure 4.1 an example for the variable ROS, requires to: select the variable to be analysed in the Variable (select one) box choose the type of alternative hypothesis to be tested in the Alternative hypothesis box; the possible values are Two sided, Less than and Greater than choose the level of confidence with the slider Confidence level finally, indicate the value to be tested under the null hypothesis in the box Comparison value Radiant automatically calculates both the confidence interval and the test. In particular, in the example above on the ROS variable, the test we perform is \\[\\begin{equation*} H_0: \\mu = 0 \\qquad \\mbox{vs.} \\qquad H_1: \\mu \\ne 0, \\end{equation*}\\] where \\(\\mu\\) indicates the unknown value of the population average for the ROS variable. Figure 4.1: Radiant page for the inference on the mean of a normal population. These results indicate that the 90% confidence interval for \\(\\mu\\) is equal to \\((6.7799, 7.7978)\\), while the test returns a very small p-value, reported as &lt;0.001, which allows to largely reject the null hypothesis even using a level of significance \\(\\alpha\\) equal to 0.01. If a one-sided test is chosen, the confidence interval returned by Radiant is not the standard (i.e. bilateral) one presented in the course and therefore we will not comment it here. "],
["inference-on-the-proportion-of-successes-in-a-bernoulli-population.html", "4.2 Inference on the proportion of successes in a Bernoulli population", " 4.2 Inference on the proportion of successes in a Bernoulli population In analogy to the case of the mean of a normal population, Radiant provides the menu Basics \\(\\rightarrow\\) Single proportion for the inference on the proportion of successes in a Bernoulli population. In the corresponding page you need to: select the variable to be analysed in the Variable (select one) box indicate the category that represents the “success” in the box Choose level choose the type of alternative hypothesis to be tested in the Alternative hypothesis box; possible values are Two sided, Less than and Greater than choose the confidence level with the slider Confidence level finally, indicate the value to be tested under the null hypothesis in the box Comparison value In this case too, Radiant automatically calculates both the confidence interval and the proportion test. Suppose we want to estimate the proportion of CEO having an MBA in the reference population using the data available in the forbes94 data frame. In addition, we also want to calculate the 99% confidence interval and we are interested in testing the null hypothesis that the proportion of CEO having an MBA in the population is equal to 30%. The results, shown in Figure 4.2, indicate that the 99% confidence interval is equal to \\((0.2257, 0.3057)\\), while the p-value of the test, equal to 0.027, suggests that there exists sufficient empirical evidence coming from the data to reject the null hypothesis \\(H_0: p = 0.30\\) at a significance level \\(\\alpha\\) of 0.05, but not at 0.0137. Figure 4.2: Radiant page for the test on a single proportion. These values do not correspond exactly to the calculation we presented during the course, because Radiant uses a slightly different but more precise formula. Remember, in fact, that the expressions we have seen for this situation are approximations based on the normal asymptotic distribution, while Radiant uses what is called an exact test (binomial exact), which hence can be used also with small sample sizes.↩ "],
["inference-on-the-comparison-between-the-means-of-two-normal-populations.html", "4.3 Inference on the comparison between the means of two normal populations", " 4.3 Inference on the comparison between the means of two normal populations The case of the difference between two means is among the most frequently used in the applications and in the course we presented some variations, in particular: comparison between the means of two normal populations with known variances, independent samples comparison between the means of two normal populations with unknown but equal variances, independent samples comparison between the means of two normal populations with unknown variances, dependent samples In Radiant there are no tools to address the first case, so we will not present it here. Moreover, for the second situation, Radiant only offers the possibility to compare two means when the variances are not known and different. This situation has not been discussed during the course because the calculations required are challenging. However, since now we have Radiant at our disposal, we will discuss this case as well. 4.3.1 Independent samples To compare the means of two normal populations with unknown variances in Radiant we can use the menu Basics \\(\\rightarrow\\) Compare means, but with respect to the case of a single mean we now have to provide the following inputs: in the Select a factor or numeric variable box we must indicate the variable (typically a factor) that identifies the groups we want to compare in the box Numeric variable we must select the numerical variable whose means we want to compare in the box Alternative hypothesis we indicate the type of alternative hypothesis we want to test; possible values are Two sided for the two-side test and Less than or Greater than for the one-side tests the slider Confidence level allows us to choose the confidence level the checkbox Show additional statistics allows us to print additional results (for example, confidence intervals, which hence are not automatically offered) we must verify that the independent button is selected in the Sample type section we must finally verify that the t-test button is selected in the Test type section Let’s look at an example in which we compare the means of the ROS variable in the two groups identified by the MasterPhd variable, which indicates the CEOs holding a Master’s or PhD degree in 1994. We calculate the confidence interval at the 95% level and we carry out the test to check if the means of the respective populations are equal, i.e. \\[\\begin{equation*} H_0: \\mu_{(\\mbox{MasterPhd}=0)} = \\mu_{(\\mbox{MasterPhd}=1)} \\quad \\mbox{vs.} \\quad H_1: \\mu_{(\\mbox{MasterPhd}=0)} \\ne \\mu_{(\\mbox{MasterPhd}=1)}. \\end{equation*}\\] The results are reported in Figure 4.3. Figure 4.3: Radiant page for the comparison of two means (independent samples). The sample mean in the second sample (MasterPhd = 1) turns out to be greater than that in the first sample (MasterPhd = 0) and the test results suggest that there appear to be significant differences between the ROS averages in the two population of companies, those whose CEO does not have a Masters or a PhD and those in which the CEO has a Masters or a Phd, but only if we decide to use a level of significance of 10% (the p-value of the test is in fact equal to 0.0838). The Plot tab of the same menu allows us to display the results in graphical form through different types of diagrams (Scatter, Box, Density, Bar). Figure 4.4 shows some of these graphs for the previous example. Figure 4.4: Radiant page for the comparison of two means (independent samples). 4.3.2 Dependent samples A further situation that can be encountered in practice is when the means to be compared concern the same population which either has been observed at two different times or, more generally, under two different conditions. This is the case of dependent samples, since in this context the same sample is observed twice. The same case also includes the situation in which the two samples, although different, have been “matched” in such a way as to approximate as precisely as possible the situation of a single sample that has been repeatedly observed. In both contexts (same sample observed twice or two paired samples), the samples should contain the same number of observations. This case can also be managed in Radiant through the command Basics \\(\\rightarrow\\) Compare means described in the previous section, except that now we have to select the checkbox named paired. Let’s see an example using the data contained in the data frame supermarket in the same file. The data frame contains the number of customers who visited a sample of 10 stores of a supermarket chain in two different days, one in which a given promotion was active and the other where the promotion was no longer available. In particular, the data frame contains the following variables: store, which indicates the store to which the observation refers customers, which indicates the number of customers who visited each of the stores in the two days program, which instead indicates the day in which the promotion was active The goal of the example is to evaluate whether the promotion was effective in terms of increasing the average number of customers who visited the stores. We will calculate both the 90% confidence interval and the following test \\[\\begin{equation*} H_0: \\mu_X - \\mu_Y \\ge 0 \\qquad \\mbox{vs.} \\qquad H_1: \\mu_X - \\mu_Y &lt; 0, \\end{equation*}\\] where \\(X\\) indicates the population of stores where the promotion is not active, while \\(Y\\) denotes the population of stores where the promotion is active. We load the data and choose the command Basics \\(\\rightarrow\\) Compare means: in the Select a factor or numeric variable box we choose program as the variable that identifies the two samples in the box Numeric variable we select the numerical variable customers in the box Alternative hypothesis we indicate the type of alternative hypothesis we want to test; in particular, in this example we choose Less than because we are interested in checking if the data suggest that the promotion has increased the average number of visits to the stores the slider Confidence level allows us to indicate the confidence level that we are interested in using the checkbox Show additional statistics allows us to print additional results we must check that the paired button in the Sample type section is selected we must finally check that the t-test button in the Sample type section is selected Figure 4.5 shows the analysis results: Figure 4.5: Radiant page for the comparison between two means (dependent samples). Results indicate that, assuming to use a significance level of 5%, the promotion appears to have had a significant effect on the average number of visits since the p-value of the test (0.033) is less than 0.05. We conclude this section by highlighting that the confidence interval provided by Radiant in these results is one-sided, so we do not comment it any further. Recall that Radiant includes only the case of unknown and different variances.↩ "],
["inference-on-the-linear-correlation-index.html", "4.4 Inference on the linear correlation index", " 4.4 Inference on the linear correlation index The linear correlation index is the main tool for evaluating the strength of the linear association between two numerical variables. We have already discussed how to perform a correlation analysis in Radiant through the command Basics \\(\\rightarrow\\) Correlation. Figures 2.13 and 2.14 in Section 2.3.2 show the results for the linear association among Age, Salary, Bonus and Other. For example, these results show that the sample correlation index between the variables Age and Salary, equal to 0.24, indicates a weak linear association, but the low value of the p-value shows that this association is highly significant. "],
["goodness-of-fit-test-fully-specified-probabilities.html", "4.5 Goodness-of-fit test (fully specified probabilities)", " 4.5 Goodness-of-fit test (fully specified probabilities) The goodness-of-fit test allows to evaluate if an observed probability distribution for a discrete random variable (with finite support) can be considered as “compatible” with a theoretical probability distribution defined a priori. Radiant allows us to perform this test with the command Basics \\(\\rightarrow\\) Goodness of fit. In the corresponding page you need to provide the following information: the variable in the data set containing the data to be analysed (box Select a categorical variable) the probabilities that define the theoretical distribution with which to compare the observed one (box Probabilities); these probabilities must be entered as numbers in the range 0 and 1, whose sum must be equal to 1 and whose number must be equal to the number of categories observed for the variable indicated in the previous box the type of output to show (box Observed for the observed frequencies, Expected for those expected under the null hypothesis, Chi-squared for the contribution of each category to the calculation of the \\(\\chi^2\\) index39) Consider again the data frame forbes94 and in particular the variable MasterPhd. Let’s try to test the null hypothesis that in the population from which these CEOs come, the percentage share of them with and without a Master or a PhD are the same, that is 50-50%. Figure 4.6 shows the input page and the corresponding output. Figure 4.6: Radiant page for the goodness-of-fit test. The output shows a p-value equal to 0.777, i.e. the data do not allow to discard the null hypothesis and therefore there is not enough evidence against the statement that the proportion of CEOs with a Master or a PhD is different from 50%40. Let’s now look at a second example: the market_share.RData file contains a data frame with the same name, which includes a single variable called Brand. This column collects the brand of a certain product purchased from 400 customers of a wholesaler. These data refer to customers in a new sales area. The wholesaler wants to assess whether the stock policy of the 4 brands that he has used in his historical sales area can be replicated in the new area as well. To do so, the wholesaler wants to compare the observed distribution for the sample of 400 customers from the new area with the preferences of the customers from the historical area, i.e. 20% for brand A, 35% for brand B, 30% for brand C and 15% for brand D. The frequency distribution of the brands purchased by the customers included in the sample is shown in Figure 4.7. Figure 4.7: Frequency distribution of the brands purchased by customers included in the market_share data frame. After selecting the Brand variable in the Select a categorical variable box and entering the values .2 .35 .3 .15 in the Probabilities box, you can print different partial results for the goodness-of-fit test (see Figure 4.8). Figure 4.8: Radiant page for the goodness-of-fit test. Using a significance level of 5% we can conclude that the data (p-value = \\(0.032&lt;0.05\\)) support the conclusion that the preference distribution in the new area is not compatible with that of the historical area and therefore it is not advisable to replicate the same stock policy. We conclude this section by pointing out that Radiant always reports in the output of the goodness-of-fit test an assessment of the assumptions on which the test is based. For the previous example, in fact, it reports that 0.0 % of cells have expected values below 5, therefore the approximation based on which the p-value is calculated can be considered acceptable. The screenshot also allows us to view other results, referred to as Deviation std., which have not been explained in the course and which we won’t discuss here either.↩ Note that the p-value obtained is in fact equal to what we would get with a test on a single proportion, because in this case the variable we are considering takes only two categories.↩ "],
["independence-test-in-a-two-way-table.html", "4.6 Independence test in a two-way table", " 4.6 Independence test in a two-way table In the final section of this chapter we present a further test that in a certain sense extends the goodness-of-fit test presented in the previous section to the case of a vector of two discrete random variables. However, in this case the theoretical distribution with which the empirical one is compared to regards a particular situation, that of statistical independence of the two variables (null hypothesis). Radiant allows to perform the independence test for a two-way table with the command Basics \\(\\rightarrow\\) Cross-tabs. The information we need to provide in this case are: the variables for which we want to perform the independence test (two subsequent boxes, both called Select a categorical variable; we recommend selecting the row variable in the first box and the column variable in the second) the type of output to show, i.e. the observed frequencies (Observed), those expected under the null hypothesis of independence (Expected), the contributions to the calculation of the index \\(\\chi^2\\) of each cell in the table (Chi-squared), the square roots of the same contributions (Deviation std.), the conditional frequencies by rows, the conditional frequencies by columns and the relative frequencies (respectively Row percentages, Column percentages and Table percentages). Figures 4.9 and 4.10 report the output for the independence test between the variables MasterPhd and WideIndustry. Figure 4.9: Radiant page for the independence test (observed frequencies). Figure 4.10: Radiant page for the independence test (expected frequencies). The p-value of the test reported in the output (&lt;.001) allows us to conclude that there seems to be sufficient empirical evidence against the null hypothesis of independence between the two variables. In other words, we can conclude that the proportion of CEO with a Master or a PhD depends in some way on the industry where the company operates (to confirm this result, have a look at the conditional frequencies by rows). As for the goodness-of-fit test, also for the independence test Radiant reports an assessment of the test assumptions. For examples, regarding the previous example we see that 0.0 % of cells have expected values below 5, therefore the approximation based on which the p-value is calculated can be considered acceptable. A small technical note on the p-value reported in the output for this test: when one or more of the expected frequencies are small, that is 5 or less, Radiant proceeds to calculate the p-value using a simulation approach. These cases are reported in the output with the sentence p.value for chi-squared statistics obtained using simulation (2,000 replicates). This value of the p-value does not match what we would get using the chi-square distribution, but usually they should not differ much. "],
["chapter5.html", "Chapter 5 The linear regression model", " Chapter 5 The linear regression model In this chapter we present one of the most important topics of the course from a practical point of view, namely the linear regression model. We start by illustrating how to estimate a simple model, that is a model with a single predictor, and then proceed with the more general multiple model, which instead can include any number of predictors. Then, we show how to compute the predictions for an estimated model, and then we conclude with an assessment of influential observations. In this chapter we will only use the R code because the tools provided by Radiant are too limited for this important topic. Moreover, the code presented in this chapter represents the starting point for more complex analyses that you may implement in other future courses. "],
["simple-reg.html", "5.1 The simple linear regression model", " 5.1 The simple linear regression model As we discussed in the course, the simple linear regression model is defined by \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad i=1,\\ldots,n, \\tag{5.1} \\end{equation}\\] where \\((x_i, y_i)\\) represent the pairs of sample observations, \\(\\beta_0\\) and \\(\\beta_1\\) are the unknown coefficients that are supposed to link the \\(X\\) and \\(Y\\) variables in the population, while \\(\\varepsilon_i\\) corresponds to the random error which intuitively describes the information about \\(Y\\) that cannot be explained through a linear function of \\(X\\). In addition to equation (5.1), the simple linear regression model is also defined by the following assumptions: the \\(x_i\\) values are fixed numbers, or they are realizations of a random variable \\(X\\) that is independent of the errors terms, \\(\\varepsilon_i\\). In the latter case inference is carried out conditionally on the observed values of \\(X\\) the error terms, \\(\\varepsilon_i\\), are random variables with mean 0 and variance \\(\\sigma^2\\) which is constant for every \\(i=1,\\ldots,n\\) (homoskedasticity) the random error terms, \\(\\varepsilon_i\\), are uncorrelated with each other the random errors, \\(\\varepsilon_i\\), are normally distributed The main function for estimating a linear regression model in R is lm(), which stands for linear model. The first argument of this function is a type of object we never encountered so far called a formula. Formula objects are used in R to specify a relationship between a dependent variable y and one (or more) independent variables x. To specify a formula object it is necessary to use a special symbol, the tilde “\\(\\sim\\)”, which is not always easy to retrieve on the Italian keyboards41. The particular combination of keys that you need to use to get this symbol depends on the operating system (Mac, Windows, etc.), the type of machine you are using (laptops, desktops, etc.), as well as the particular brand of your machine. For this reason, it is not possible to indicate here unequivocally how to obtain the tilde on your particular computer, but we suggest an unpretentious but very simple way to get it: execute the code ?tilde, copy the tilde character in the help page and paste it where you need it42. The general syntax of the lm() function is lm(y ~ x), which returns a list (see Section 1.3.6) containing a large number of results related to the estimated model. As an example, let’s consider the data frame comp available in the competitors.RData file. The data frame comp contains the values measured in 113 consecutive weeks for the following variables related to a company that produces a food product: mktshare indicates the market share (in relative terms) of the company at the end of each week ownprice indicates the price (in $) of the product offered by the company in question at the end of each week comp1price indicates the price (in $) of the same product offered by a company competitor at the end of each week comp2price indicates the price (in $) of the same product offered by a second company competitor at the end of each week We want to estimate the linear relationship between the company’s market share in the various weeks and the price of its product in the same weeks, that is, we are interested in estimating the model \\[\\begin{equation} \\mbox{mktshare}_i = \\beta_0 + \\beta_1 \\mbox{ownprice}_i + \\varepsilon_i. \\tag{5.2} \\end{equation}\\] After loading the data (see Section 1.8), you can use the View() function to have a look at them. The following code uses the lm() function to estimate the model. The results are stored in the comp_m1 object, which can be summarized using the summary() function: &gt; load(&quot;competitors.RData&quot;) &gt; comp_m1 &lt;- lm(mktshare ~ ownprice, data = comp) &gt; summary(comp_m1) Call: lm(formula = mktshare ~ ownprice, data = comp) Residuals: Min 1Q Median 3Q Max -0.017151 -0.009612 -0.003100 0.006425 0.050105 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.110433 0.014983 7.371 3.24e-11 *** ownprice -0.045318 0.008956 -5.060 1.67e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.01293 on 111 degrees of freedom Multiple R-squared: 0.1874, Adjusted R-squared: 0.1801 F-statistic: 25.6 on 1 and 111 DF, p-value: 1.674e-06 Please note that in addition to the formula argument mktshare \\(\\sim\\) ownprice, which specifies the model to be estimated, we also added a second argument in the lm() function called data. This argument is used to provide R with the data frame where the variables specified in the formula are found43. The summary() function applied to the list returned by the lm() produces an output consisting of the following parts: the first line of the output (Call) reports the code that has been executed the second part of the output (Residuals) provides a summary of the model’s residuals the third part (Coefficients) provides the estimates of the model’s coefficients and the corresponding p-values. For the previous example, the estimated model is given by \\[\\begin{equation} \\widehat{\\mbox{mktshare}}_i = 0.1104 -0.0453 \\, \\mbox{ownprice}_i. \\tag{5.3} \\end{equation}\\] the fourth and last part (last three lines) contain some useful indices to evaluate the overall goodness of the model, in particular the estimate of \\(\\sigma\\) (referred to as residual standard error), the \\(R^2\\) index (reported as Multiple R-squared), and the \\(F\\) test (last line of the output) The \\(R^2\\) index (0.1874) indicates a limited predictive ability of the model, even if the low p-value of the coefficient \\(\\beta_1\\) (1.67e-06) suggests that the linear association between the two variables is significant from a statistical point of view. It is possible to display the fitted line directly on the scatter plot of the two variables using the following code44: &gt; plot(mktshare ~ ownprice, data = comp) &gt; abline(comp_m1, lwd = 2, col = &quot;blue&quot;) Figure 5.1: Scatter plot of mktshare versus ownprice with its corresponding estimated line. The first line of code uses the plot() function, but unlike what we have seen previously, in this case we have indicated the variables to be represented in the graph through a formula. The second line of code uses the abline() function, which adds a line to the active chart, in this case the estimated line. The output provided by R for a linear regression model includes the p-values for the tests on the two coefficients, but does not include the corresponding confidence intervals. To get these, you need to use the confint() function directly on the result of lm(). For the previous example we have: &gt; confint(comp_m1) 2.5 % 97.5 % (Intercept) 0.08074335 0.14012364 ownprice -0.06306539 -0.02757108 Other useful functions to inspect the results of a linear regression model (not necessarily a simple one) are: coef(), which prints the coefficient estimates residuals(), which calculates the model residuals fitted(), which calculates the predicted values of the model only for the observations included in the sample anova(), which produces the deviance decomposition predict(), which allows us to calculate predictions Leaving aside for the moment predict(), to which we dedicate a subsequent section of the chapter, we now see the type of output produced by the other functions mentioned above: &gt; coef(comp_m1) (Intercept) ownprice 0.11043350 -0.04531824 &gt; anova(comp_m1) Analysis of Variance Table Response: mktshare Df Sum Sq Mean Sq F value Pr(&gt;F) ownprice 1 0.0042783 0.0042783 25.604 1.674e-06 *** Residuals 111 0.0185475 0.0001671 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The following code saves in two vectors the predicted values for the observations in the sample and the corresponding residuals, which we remind are defined as the differences between the observed and those predicted by the model. These quantities can be represented in a graph, known as the residual graph, which represents a fundamental tool for assessing the goodness of the model (i.e. the assumptions regarding errors): &gt; comp_m1_fitted &lt;- fitted(comp_m1) &gt; comp_m1_resid &lt;- residuals(comp_m1) &gt; plot(comp_m1_fitted, comp_m1_resid, xlab = &quot;Fitted values&quot;, ylab = &quot;Residuals&quot;) &gt; abline(h = 0, lty = 2) Figure 5.2: Plot of residuals versus predicted values for model comp_m1. The h and lty options in the previous code allow us to draw an horizontal line (in correspondence of the value 0 on the vertical axis) and to choose the line type (the value 2 indicates a dotted line) respectively. We use the tilde symbol because R is a software born and developed in the Anglo-Saxon countries and on the US and UK keyboards this symbol is directly available as one of the keys.↩ In general, on Mac computers you should be able to retrieve the tilde symbol with the combination of keys alt+5, i.e. the option key, usually located on the left of the space bar, along with the number 5. On most desktop computers (but not laptops) with a Windows operating system, it is possible to retrieve the tilde with the key combination Alt+126, i.e. the Alt key (usually located on the left of the space bar) together with the number 126. The number 126 must be entered using the numeric keypad (i.e. do not use the numbers placed above the central part of the keyboard). Unfortunately, the position of this symbol on laptops depends on the keyboard and the brand of the laptop itself, so we invite you to consult the instruction manual of your laptop.↩ This is necessary because the variables of a given frame are not available as self-contained objects in R memory, unless you previously attach()-ed the data frame.↩ The lwd option allows to choose the thickness of the line, while col allows to choose its color.↩ "],
["the-multiple-linear-regression-model.html", "5.2 The multiple linear regression model", " 5.2 The multiple linear regression model In most applications there are many variables available in the data set that can help explaining the observed values of the dependent variable, which might allow obtaining models with a better predictive ability. The tool typically used in these cases is the multiple linear regression model, which differs from the simple one because it allows to include any number of predictors in the model. The multiple linear regression model is defined as \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\cdots + \\beta_K x_{Ki} + \\varepsilon_i, \\quad i=1,\\ldots,n, \\tag{5.4} \\end{equation}\\] where the notation has the same meaning as for the simple linear regression model. In addition to equation (5.4), the multiple linear regression model is also based on the following assumptions: the \\(x_{ji}\\) terms are fixed numbers, or they are realizations of a random variable \\(X_j\\) (\\(j=1,\\ldots,K\\)), that are independent of the error terms, \\(\\varepsilon_i\\). In the latter case, inference is carried out conditionally to the observed values of \\(X_j\\) the error terms, \\(\\varepsilon_i\\), are random variables with mean 0 and variance \\(\\sigma^2\\) which is constant for every \\(i=1,\\ldots,n\\) (homoskedasticity) the error terms, \\(\\varepsilon_i\\), are uncorrelated with each other there are no linear relationships between the predictor variables the random error terms, \\(\\varepsilon_i\\), are normally distributed The functions available in R to estimate and analyse a multiple linear regression model are exactly the same as those presented in Section 5.1, except that now the formula to be specified within the lm() will use more than one variable to the right of the tilde symbol. The independent variables that we want to include in the model must be added one after the other using the + symbol. For example, to generalize the simple linear regression model comp_m1 presented above, we now also include the variables related to the competitors’ prices: &gt; comp_m2 &lt;- lm(mktshare ~ ownprice + comp1price + comp2price, data = comp) &gt; summary(comp_m2) Call: lm(formula = mktshare ~ ownprice + comp1price + comp2price, data = comp) Residuals: Min 1Q Median 3Q Max -0.017958 -0.005826 -0.001982 0.004266 0.039360 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.040070 0.014050 2.852 0.00520 ** ownprice -0.076424 0.007957 -9.604 3.42e-16 *** comp1price 0.026333 0.008387 3.140 0.00218 ** comp2price 0.045968 0.007817 5.880 4.55e-08 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.009965 on 109 degrees of freedom Multiple R-squared: 0.5258, Adjusted R-squared: 0.5127 F-statistic: 40.29 on 3 and 109 DF, p-value: &lt; 2.2e-16 The estimated model is then given by \\[\\begin{equation} \\widehat{\\mbox{mktshare}}_i = 0.0401 -0.0764 \\, \\mbox{ownprice}_i + 0.0263 \\, \\mbox{comp1price}_i + 0.0460 \\, \\mbox{comp2price}_i. \\tag{5.5} \\end{equation}\\] The output is structured exactly in the same way, except that now the table providing the estimated coefficients (Coefficients) contains a larger number of rows, each corresponding to one of the coefficients of the model. The extractor functions described for the simple linear regression model (coef(), residuals(), etc.) also apply to the multiple linear regression model: &gt; coef(comp_m2) (Intercept) ownprice comp1price comp2price 0.04006985 -0.07642379 0.02633343 0.04596776 &gt; anova(comp_m2) Analysis of Variance Table Response: mktshare Df Sum Sq Mean Sq F value Pr(&gt;F) ownprice 1 0.0042783 0.0042783 43.083 1.832e-09 *** comp1price 1 0.0042896 0.0042896 43.197 1.757e-09 *** comp2price 1 0.0034339 0.0034339 34.580 4.549e-08 *** Residuals 109 0.0108240 0.0000993 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; comp_m2_fitted &lt;- fitted(comp_m2) &gt; comp_m2_resid &lt;- residuals(comp_m2) &gt; plot(comp_m2_fitted, comp_m2_resid, xlab = &quot;Fitted values&quot;, ylab = &quot;Residuals&quot;) &gt; abline(h = 0, lty = 2) Figure 5.3: Plot of residuals versus predicted values for model comp_m2. "],
["how-to-compute-the-predictions-of-a-linear-regression-model.html", "5.3 How to compute the predictions of a linear regression model", " 5.3 How to compute the predictions of a linear regression model One of the reasons for the popularity of the linear regression model is its ability to calculate predictions in relation to scenarios that have not necessarily been observed in the sample. During our course we have seen that there are two types of predictions, that for the mean value of the dependent variable and that for the individual value. They differ in their standard errors, which is larger for the second type of predictions than for the first. R allows to calculate predictions for a linear regression model through the predict() function, which requires the following arguments to be specified: object, which is the object returned by the lm() function containing the estimation results newdata, which refers to a new data frame containing the values of the independent variables to use in the calculation of the predictions interval, which can take only three possible values, that is confidence if we want to calculate the confidence intervals for the prediction of the mean value, prediction if we want to calculate the prediction intervals for individual values, none if no interval is to be calculated level, which allows us to specify the confidence level for the confidence or prediction intervals Attention should be paid in using the predict() function, especially in correctly specifying the newdata data frame. This data frame can contain an arbitrary number of lines, each of which refers to a scenario (that is, a combination of values of independent variables) in correspondence to which the prediction is to be calculated, but it must contain the same number of columns as those included in the model. The columns of newdata must be named with the same names as the variables included in the model. If variable names do not match, R will not be able to associate the variables included in the model with those provided in the newdata data frame. For example, suppose that, with reference to the model comp_m2, we are interested in calculating the predictions and the corresponding 95% prediction intervals for the following two scenarios45: &gt; data_for_predict &lt;- data.frame(ownprice = c(1.60, 1.80), + comp1price = c(1.65, 1.90), + comp2price = c(1.50, 1.75)) &gt; data_for_predict ownprice comp1price comp2price 1 1.6 1.65 1.50 2 1.8 1.90 1.75 The data frame data_for_predict then contains two lines, one for each scenario, and three columns whose names match those of the data frame containing the sample data (i.e. comp). Now let’s use the predict() function to compute the predictions: &gt; predict(object = comp_m2, newdata = data_for_predict, + interval = &quot;prediction&quot;, level = 0.95) fit lwr upr 1 0.03019360 0.01020255 0.05018465 2 0.03298414 0.01296260 0.05300567 The output returned by predict() consists of a matrix with two rows corresponding to the two scenarios and three columns containing the predictions (fit) and the prediction interval boundaries (lwr and upr). The sign + you see at the beginning of the second and third code lines is added by R and indicates that the code in the first line continues on the following lines, but you do not have to write it when you’ll reproduce the code.↩ "],
["influential-observations.html", "5.4 Influential observations", " 5.4 Influential observations Once we estimate the linear regression model, we need to perform a series of checks to verify that the model’s assumptions are fulfilled and to evaluate that there are not too many observations that exert a particularly strong influence on the results. We now present some features available in R to identify the latter. Two R functions useful for identifying the presence of potentially influential observations are rstandard() and hatvalues(), which respectively return the standardized residuals and the so-called leverages. Both functions require only the estimated model as the argument and they output a numeric vector containing the corresponding quantities. Both these measures can be represented in a graph to highlight the most critical observations. The following code computes the standardized residuals and produces the corresponding plot for the multiple model comp_m2: &gt; stdres &lt;- rstandard(comp_m2) &gt; plot(stdres, ylim = c(min(stdres, -3), max(stdres, 3)), + xlab = &quot;Row number&quot;, ylab = &quot;Standardized residuals&quot;, type = &quot;n&quot;) &gt; text(x = stdres, labels = 1:nrow(comp), cex = 0.5) &gt; abline(h = c(-2, 0, 2), lty = 2) Figure 5.4: Plot of standardized residuals for model comp_m2. Note that in the plot() function we used the argument type = “n” (which stands for “nothing”), which allows us to prepare the axes of the graph, but not to draw the points. These are added with the next code line through the text() function, which superimposes a text label in correspondence of the point coordinates (x = stdres). In this example, the text labels used are the row numbers (labels = 1:nrow(comp)). This allows to identify directly from the graph the observations with a particularly large standardized residual (in absolute value). The thresholds beyond which the observations are usually classified as outliers are -2 and +2 (these values are added to the graph by means of the abline() function). In this case there are 7 outliers, namely observation number 2, 3, 45, 71, 89, 103 and 113. The following code calculates and produces the leverage plot for the same model: &gt; lev &lt;- hatvalues(comp_m2) &gt; lev_thr &lt;- 2*mean(lev) &gt; plot(lev, ylim = c(0, max(lev, lev_thr)), + xlab = &quot;Row number&quot;, ylab = &quot;Leverage&quot;, type = &quot;n&quot;) &gt; text(x = lev, labels = 1:nrow(comp), cex = 0.5) &gt; abline(h = lev_thr, lty = 2) Figure 5.5: Plot of leverages for model comp_m2. For leverages, the commonly used threshold is twice the average of the leverages (which in the previous code is stored in the object lev_thr). For this model there are 7 observations with high leverage, namely observation number 2, 88, 93, 94, 96, 104 and 108. "],
["appendix.html", "Chapter 6 Appendix", " Chapter 6 Appendix In this appendix we present some more in-depth presentation not required for the course 30001 (Statistics), but which allow to get more detailed outputs than those provided by Radiant. These additional information can be useful for future courses in which R is used or for analysis of data sets related to the “triennio” final project as well as the “biennio” thesis. "],
["some-r-functions-for-descriptive-statistics.html", "6.1 Some R functions for descriptive statistics", " 6.1 Some R functions for descriptive statistics 6.1.1 Univariate descriptive analyses 6.1.1.1 A single categorical variable If we are interested in summarizing the information corresponding to a categorical variable, the tools we can use are the frequency distribution together with a graphical representation of it. Before showing some examples, we remind you that in R categorical variables correspond to factor vectors (see Section 1.3.4). If the variable you want to analyse is not coded as a factor, you must first con-ert it using the as.factor() function. For example, to convert the numerical variable IndustryCode in the data frame forbes94 we need to execute the following code: &gt; load(&quot;Forbes94.RData&quot;) &gt; forbes94$IndustryCode &lt;- as.factor(forbes94$IndustryCode) &gt; str(forbes94$IndustryCode) Factor w/ 20 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 12 12 12 12 12 12 12 12 12 12 ... Remember that the $ operator is used to refer to a column in a data frame. To construct the frequency distribution of a categorical variable in R we can use the table() function, already described in Section 2.1. For example, the frequency distribution of the categorical variable WideIndustry is obtained with the following code: &gt; table(forbes94$WideIndustry) Aerospacedefense Business Capital goods Chemicals 19 27 19 25 ComputersComm Construction Consumer Energy 67 11 54 42 Entertainment Financial Food Forest 27 168 62 19 Health Insurance Metals Retailing 49 54 18 46 Transport Travel Utility 15 15 63 As always, it is possible to store the result of the table() function (a numeric vector containing the absolute frequencies whose elements are labels with as categories of the variable) in a new object and use it for further analysis. To produce the relative frequencies we can use the prop.table() function, which requires as input the table with the absolute frequencies: &gt; absfreq &lt;- table(forbes94$WideIndustry) &gt; prop.table(absfreq) Aerospacedefense Business Capital goods Chemicals 0.02375 0.03375 0.02375 0.03125 ComputersComm Construction Consumer Energy 0.08375 0.01375 0.06750 0.05250 Entertainment Financial Food Forest 0.03375 0.21000 0.07750 0.02375 Health Insurance Metals Retailing 0.06125 0.06750 0.02250 0.05750 Transport Travel Utility 0.01875 0.01875 0.07875 Once the frequency distribution is available, we can represent it graphically using a pie or bar chart. In R these are obtained respectively with the pie() (already described in Section 2.1) and barplot() functions, both requiring the (either absolute or relative) frequencies: &gt; pie(absfreq, main = &quot;Distribution of WideIndustry&quot;) # pie chart &gt; barplot(absfreq, main = &quot;Distribution of WideIndustry&quot;) # bar chart Figure 6.1: Pie and bar charts with R. 6.1.1.2 A single numerical variable 6.1.1.2.1 Frequency distribution The distribution of a numerical variable is represented by a table that shows the frequencies with which the different values are observed, if the variable is discrete, or the classes in which it is divided, if the variable is continuous. For both cases, it is still possible to use the table() function, but for continuous variables it is first necessary to split the observed values in classes. A useful function for generating these classes is cut(), which creates a factor with the specified classes and which includes the following arguments: x, the numeric vector whose values we want to split in classes breaks, which allows to specify which classes we want to use; it can be specified in the following alternative ways a single numeric value that indicates the number of equal-width classes we want to use a vector of numeric values that identify the extremes of the classes (possibly of different widths) to be used right, a logical value that indicates whether the classes should be closed on the left (FALSE) or on the right (TRUE) labels, a vector of labels to be used to identify the classes; if it is set to FALSE, the classes will be identified by integers For example, suppose we want to construct the frequency distribution of the Salary variable, which provides the salaries paid in 1994 to the CEOs in the data set, using 5 equal-width classes closed on the left. The following code allows us to create a new variable, that we call Salary_c and add to the existing data frame: &gt; forbes94$Salary_c &lt;- cut(forbes94$Salary, breaks = 5, right = FALSE) &gt; table(forbes94$Salary_c) [1.59e+04,5.55e+05) [5.55e+05,1.09e+06) [1.09e+06,1.63e+06) 382 376 24 [1.63e+06,2.16e+06) [2.16e+06,2.7e+06) 4 3 from which we see that the distribution of salaries for these CEOs is decidedly right skewed. If we wanted to use the \\([0, 200000]\\), \\([200000, 500000]\\), \\([500000, 800000]\\), \\([800000, 1500000]\\) and \\([1500000, 3000000]\\) classes, then we need to use the cut() function as follows: &gt; forbes94$Salary_c &lt;- cut(forbes94$Salary, + breaks = c(0, 200000, 500000, 800000, 1500000, 3000000), right = FALSE) &gt; table(forbes94$Salary_c) [0,2e+05) [2e+05,5e+05) [5e+05,8e+05) [8e+05,1.5e+06) 13 268 351 147 [1.5e+06,3e+06) 10 To represent graphically the distribution of a numerical variable, various types of graphs are available including the spike plot, the histogram and the box-plot. 6.1.1.2.2 Spike plots We can create a spike plot using the plot() function directly on the result provided by table() and setting the type argument equal to “h”. For example, the spike plot for the Age variable is obtained as follows: &gt; plot(table(forbes94$Age), type = &quot;h&quot;) Figure 6.2: Spike plot for the Age variable. The final appearance of the graph can be changed modifying the many arguments of the plot() function. For example, if we want to add the labels to the axes, we should use the arguments xlab and ylab, that is &gt; plot(table(forbes94$Age), type = &quot;h&quot;, + xlab = &quot;Age&quot;, ylab = &quot;Absolute frequency&quot;, + main = &quot;Spike plot of the Age variable&quot;) Figure 6.3: Spike plot for the Age variable. 6.1.1.2.3 Histogram Histograms are graphical representations of the frequency distribution of a variable divided by classes. In R it is possible to create a histogram with the function hist(). As you already known, it is possible to create a histogram using classes of equal or different widths. The hist() function allows to specify our choice with the breaks argument, whose usage is identical to that we described for the cut() function, that is: if we do not specify the value of breaks, R uses some internal rules to choose a reasonable value for it; we recommend using this option, because it works well in most cases, unless there are special reasons for using a different approach breaks can be specified as a single numerical value (a positive integer), which will indicate the number (indicative46) of classes of equal width we want to use breaks can also be specified as a numeric vector, whose elements correspond to the extremes of the classes we intend to use In addition to breaks, the hist() function contains many other arguments with which you can modify the final look of the graph, such as main, xlab and ylab, which work as we have seen so far. Let’s now look at some histogram examples for the Salary variable: &gt; hist(forbes94$Salary, main = &quot;Histogram of Salary&quot;, xlab = &quot;Salary (in $)&quot;, + ylab = &quot;Absolute frequency&quot;) Figure 6.4: Example of histogram for the Salary variable (equal-width classes). &gt; hist(forbes94$Salary, main = &quot;Histogram of Salary&quot;, xlab = &quot;Salary (in $)&quot;, + ylab = &quot;Absolute frequency&quot;, breaks = 50) Figure 6.5: Example of histogram for the Salary variable (equal-width classes). &gt; hist(forbes94$Salary, main = &quot;Histogram of Salary&quot;, xlab = &quot;Salary (in $)&quot;, + ylab = &quot;Density&quot;, + breaks = c(0, 100000, 300000, 350000, 500000, 800000, 1000000, 3000000)) Figure 6.6: Example of histogram for the Salary variable (different width classes). These examples clearly show that the distribution of Salary is skewed to the right. We also point out that in the last example, where we used 7 classes of different widths, R has automatically (and correctly!) decided to report the densities on the vertical axis, while in the other two examples the vertical axis reports the absolute frequencies. You may decide to report the density on the vertical axis even when you use equal-width classes setting to FALSE the value of the argument freq47. 6.1.1.2.4 Box-plot In addition to the spike plot and histogram, it is useful also to produce the box-plot, which allows to integrate the information provided by the other graphs. As we have already seen in Section 2.1, in R it is possible to construct a box-plot using the boxplot() function. Recall that the box-plot of the Salary variable is obtained as follows: &gt; boxplot(forbes94$Salary) Figure 6.7: Example of box-plot with R. A useful final comment on the boxplot() function concerns the possibility of saving additional output regarding the graph. In particular, boxplot() allows to save the values of the indices used to construct the graph in a list (see Section 1.3.6). For example, for the variable Salary we have &gt; bp_out &lt;- boxplot(forbes94$Salary) &gt; str(bp_out) List of 6 $ stats: &#39;integer&#39; num [1:5, 1] 18600 433333 569231 750000 1200000 $ n : num 789 $ conf : num [1:2, 1] 551419 587043 $ out : num [1:18] 1750000 1400000 1500000 1375000 2000000 ... $ group: num [1:18] 1 1 1 1 1 1 1 1 1 1 ... $ names: chr &quot;1&quot; As you see, bp_out is a list containing 6 elements (i.e. other objects), including stats, which contains the values of the 5 indices needed to build the box-plot (minimum, first quartile, median, third quartile and maximum), and out, the vector of outliers. Therefore, we can conclude that the Salary variable has 18 outliers: [1] 1750000 1400000 1500000 1375000 2000000 1900000 1250000 1483500 [9] 1454000 2633570 1240000 1841000 1231730 2667730 2700000 1525000 [17] 1397810 1504940 6.1.1.2.5 Summary indices The main functions available in R for the calculation of summary indices for a numeric variable are: min() returns the minimum observed value max() returns the maximum observed value mean() returns the sample mean median() returns the sample median var() returns the sample variance sd() returns the sample standard deviation quantile() returns the sample quantiles summary(), which automatically provides a list with some of the above mentioned indices If the variable contains missing values (i.e. NA), all these functions except from summary() require the na.rm argument to be set to TRUE, with which we tell to R to remove the missing data internally (but not from the data set) before computing the indices. Let’s see an example using the Salary variable: &gt; min(forbes94$Salary, na.rm = TRUE) [1] 18600 &gt; max(forbes94$Salary, na.rm = TRUE) [1] 2700000 &gt; mean(forbes94$Salary, na.rm = TRUE) [1] 613458 &gt; median(forbes94$Salary, na.rm = TRUE) [1] 569231 &gt; var(forbes94$Salary, na.rm = TRUE) [1] 78579020522 &gt; sd(forbes94$Salary, na.rm = TRUE) [1] 280319.5 &gt; quantile(forbes94$Salary, na.rm = TRUE) 0% 25% 50% 75% 100% 18600 433333 569231 750000 2700000 &gt; summary(forbes94$Salary) Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s 18600 433333 569231 613458 750000 2700000 11 The quantile() function provides the percentiles of a numeric set of values. The order of the percentiles we want to calculate is specified with the probs argument. For example, the percentiles of order 10 and 95 for the variable Salary are computed as follows: &gt; quantile(forbes94$Salary, na.rm = TRUE, probs = c(0.10, 0.95)) 10% 95% 325148.8 1025640.0 6.1.2 Bivariate descriptive analyses 6.1.2.1 Two categorical variables In case we want to jointly analyse two categorical variables, the main tool we can use is a two-way table. In R we can construct a two-way table with the table() function, that we already encountered in other situations. To obtain a two-way table, however, we must specify two variables, respectively the row variable and the column variable (in this order). The following code constructs the two-way table for the MBA variable against WideIndustry: &gt; table(forbes94$WideIndustry, forbes94$MBA) 0 1 Aerospacedefense 15 4 Business 22 5 Capital goods 13 6 Chemicals 17 8 ComputersComm 46 21 Construction 7 4 Consumer 36 18 Energy 32 10 Entertainment 23 4 Financial 112 56 Food 45 17 Forest 12 7 Health 37 12 Insurance 43 11 Metals 14 4 Retailing 43 3 Transport 11 4 Travel 13 2 Utility 48 15 The graphical representation of a two-way table corresponds to a stacked bar chart, which is obtained with the plot() function where the variables must be specified through a formula: &gt; plot(MBA ~ WideIndustry, data = forbes94) Figure 6.8: Stacked bar chart for the MBA variable versus WideIndustry. 6.1.2.2 Two numerical variables In R it is possible to create a scatter plot with the plot() function where we specify the variable \\(x\\) as the first argument and \\(y\\) as the second one48. As for the other graphical functions, we can add other arguments with which we can modify the final appearance of the graph. The following code creates the scatter plot for the Salary variable against the Age variable: &gt; plot(forbes94$Age, forbes94$Salary, xlab = &quot;Age (in years)&quot;, + ylab = &quot;Salary (in $)&quot;, main = &quot;Scatter plot&quot;) Figure 6.9: Scatter plot of the Salary variable versus Age. For the calculation of the covariance and the linear correlation index we can use the cov() and cor() functions respectively. If we include more than two variables in these functions, they will return the sample covariance and correlation matrices respectively. If any of the variables contain missing data, you should also include the use argument and set it to “complete.obs”49. To calculate the covariances and correlations among the variables Age, Salary, Bonus and Other we can execute the following code: &gt; cov(forbes94[, c(&quot;Age&quot;, &quot;Salary&quot;, &quot;Bonus&quot;, &quot;Other&quot;)], + use = &quot;complete.obs&quot;) Age Salary Bonus Other Age 41.58899 4.059318e+05 7.083466e+05 4.464542e+05 Salary 405931.80451 6.886109e+10 6.193965e+10 9.695591e+10 Bonus 708346.60585 6.193965e+10 8.676328e+11 4.023467e+11 Other 446454.17141 9.695591e+10 4.023467e+11 1.670905e+12 &gt; cor(forbes94[, c(&quot;Age&quot;, &quot;Salary&quot;, &quot;Bonus&quot;, &quot;Other&quot;)], + use = &quot;complete.obs&quot;) Age Salary Bonus Other Age 1.00000000 0.2398706 0.1179203 0.05355646 Salary 0.23987057 1.0000000 0.2534042 0.28583240 Bonus 0.11792032 0.2534042 1.0000000 0.33416164 Other 0.05355646 0.2858324 0.3341616 1.00000000 The correlations between these variables are all positive, but weak or very weak. This is mainly due to the presence of numerous outliers and to the heterogeneity of the observed values. 6.1.3 Further graphical representations 6.1.3.1 The Pareto diagram The Pareto diagram is a graphical representation useful for separating the modalities of a categorical variable that have been observed more frequently from the less frequent ones. Unfortunately, there is no function in the basic R packages that produce a Pareto diagram. However, it is possible to download and install an additional package, called qcc, which contains some functions dedicated to the statistical quality control in production, which also includes a function to draw the Pareto diagram. We can install the package by following the instructions in Section 1.5, or alternatively by running the following code: &gt; install.packages(&quot;qcc&quot;) # download the qcc package &gt; library(qcc) # load the qcc package The qcc package contains a function called pareto.chart() that allows us to directly produce the Pareto diagram. This function requires a numeric vector with the absolute frequencies with which the various categories have been observed. To obtain a more informative graph, we also recommend to assign names to the elements of the vector, which will be shown on the horizontal axis of the graph. The following code first loads some data contained in the data frame pareto_dat, which correspond to the number of defective pieces detected in a company (column Total), together with the corresponding cause of defect (column Cause), and then use this data to construct the Pareto diagram: &gt; load(&quot;pareto_data.RData&quot;) &gt; pdat &lt;- pareto_dat$Total &gt; names(pdat) &lt;- pareto_dat$Cause &gt; pareto.chart(data = pdat, main = &quot;Pareto diagram&quot;) Figure 6.10: Example of a Pareto diagram using the pareto.chart() function from the qcc package. Pareto chart analysis for pdat Frequency Cum.Freq. Percentage Cum.Percent. Compressor 15.000000 15.000000 22.058824 22.058824 Machine adjustment 11.000000 26.000000 16.176471 38.235294 Packing 10.000000 36.000000 14.705882 52.941176 Supplier 9.000000 45.000000 13.235294 66.176471 Operation conditions 8.000000 53.000000 11.764706 77.941176 Transport agency 3.000000 56.000000 4.411765 82.352941 Measurement apparatus 3.000000 59.000000 4.411765 86.764706 Receptionist 2.000000 61.000000 2.941176 89.705882 Transport method 2.000000 63.000000 2.941176 92.647059 Recording method 2.000000 65.000000 2.941176 95.588235 Recording Operator 1.000000 66.000000 1.470588 97.058824 Storage operators 1.000000 67.000000 1.470588 98.529412 Raw materials reception 1.000000 68.000000 1.470588 100.000000 6.1.3.2 Graphical representation of a time series Another situation that is frequently encountered when analysing data is that in which the observations are temporally ordered. This type of data is called time series. The ts() function allows us to create a time series and define its characteristics in terms of the number of observations per year, starting and ending dates. As an example, we load the file advertising_sales.RData, which contains the data frame advertising_sales with the monthly historical series of sales and advertising expenditures (both in thousands of dollars) of a fitness product. The available data have been collected since January 2012. The following code allows to define the data as a times series object by applying the function ts() with the argument frequency set at 12 (i.e. 12 observations per year) and the argument start to the vector c (2012, 1), which respectively indicate the year and the month of the first observation in the sample. Finally, with the function plot() we create the time plot of the two series, which we represent with different colors and line types (arguments col and lty)50: &gt; load(&quot;advertising_sales.RData&quot;) &gt; adv_sales &lt;- ts(advertising_sales[, c(2, 3)], frequency = 12, start = c(2012, 1)) &gt; plot(adv_sales, plot.type = &quot;single&quot;, col = c(&quot;red&quot;, &quot;blue&quot;), + lwd = 2, lty = c(1, 2), ylab = &quot;&quot;, main = &quot;Time series plot&quot;, + sub = &quot;(solid line = advertising expenditures, dashed line = sales)&quot;) Figure 6.11: Example of time series plot. 6.1.3.3 How to check if a variable is normally distributed Many statistical models require that the reference population is distributed according to a normal distribution. Therefore, it is necessary to verify that this assumption is at least approximately satisfied by the sample data we are analysing otherwise the results we obtain may be biased and unreliable. A simple but effective graphical tool for verifying the normality of a distribution of a numeric set of data is the normal probability plot, which can be created in R through the qqnorm() function. This function requires only the variable to analyse, while other arguments are available to improve the appearance of the graph (main, xlab, ylab, col, etc. .). After executing qqnorm(), the qqline() function allows adding a line to the chart that helps in evaluating the proximity of the empirical distribution of the data to the normal distribution. The qqline() function also requires the name of the variable being analysed. The following code generates the normal probability plot for the Salary variable: &gt; qqnorm(forbes94$Salary, main = &quot;Normal probability plot for Salary&quot;) &gt; qqline(forbes94$Salary) Figure 6.12: Example of normal probability plot for the Salary variable. From the graph we can conclude that the distribution of Salary is skewed to the right and therefore cannot be considered distributed as normal. As a second example we consider the variable Age: &gt; qqnorm(forbes94$Age, main = &quot;Normal probability plot for Age&quot;) &gt; qqline(forbes94$Age) Figure 6.13: Example of normal probability plot for the Age variable. In this case the empirical distribution of Age is much closer, though not perfectly, to a normal distribution51. As it is also highlighted in the help of the function hist(), in fact, the value that we provide for the number of equal-width classes is only a “suggestion”, but internally this value is revised according to some rules, which is not our goal to describe now.↩ If you try to set freq = TRUE in the case of classes with different widths, you would get an instructive error message.↩ Alternatively, the variables can be specified through a formula, for example plot(Salary \\(\\sim\\) Age, data = forbes94).↩ Other choices are available for use, but we do not present them in this manual. However, we invite you to read the help pages of these functions.↩ The arguments lwd, lty and sub are used to define the thickness of the line (2 means lines twice as thick as normal), the line type (1 means a solid line, while 2 means a dashed one) and a subtitle, shown at bottom of the graph.↩ Remember that the normal probability plot is a graphical tool, therefore it helps to discriminate well if we are close or not to a situation of normality only in the most evident cases. In many situations, however, the chart does not allow to reach a clear conclusion. In these cases it is necessary to use more elaborate tools that are not discussed neither here nor in the course.↩ "],
["some-r-functions-for-inferential-statistics.html", "6.2 Some R functions for inferential statistics", " 6.2 Some R functions for inferential statistics In this section we show how to carry out with R the inferential analyses presented in the course. 6.2.1 Inference on the mean of a normal population It is possible to make inference on an mean \\(\\mu\\) of a normal population assuming that the variance \\(\\sigma^2\\) of the population is known or not. In this section we deal only with the second case because it is the most frequent and relevant situation in practice. In addition, neither R nor Radiant provide commands to deal with the case where the variance is known. In addition to the mean and sd functions already discussed in Section 6.1.1.2.5 for estimating the mean and standard deviation of a population respectively, R provides the t.test() function that allows us to manage all the cases presented in the course regarding the inference on means. The t.test() function simultaneously returns the confidence interval and the test for all cases. In the case of inference on a single mean of a normal population, the arguments to be specified are: x, variable on which to perform the analysis alternative, type of alternative hypothesis we want to use, that is two-sided (two.sided) or one-sided (less or greater, depending on the sign used in \\(H_1\\)) mu, the value of the average we want to test under \\(H_0\\) (the amount that we called \\(\\mu_0\\) in the course) conf.level, the confidence level to use Let’s see an example using again the data contained in the data frame forbes94. In particular, we define a new variable that we call ROS, or return on sales, calculated as the ratio (as a percentage) of profits and sales revenues: &gt; forbes94 &lt;- transform(forbes94, ROS = Profits/Sales*100) We now proceed to calculate the 90% confidence interval for the mean of the ROS variable. Furthermore, together with the interval we also carry out a test to verify if the data supports the conclusion that the ROS mean for the population of companies from which this sample was extracted is different from 6%. In other words, indicating with \\(\\mu\\) the unknown average of ROS in the population, the test will compare the following statements: \\[\\begin{equation*} H_0: \\mu = 6\\% \\qquad \\mbox{vs.} \\qquad H_1: \\mu \\ne 6\\%. \\end{equation*}\\] As mentioned above, all the results can be recovered in one shot by using the t.test() function as follows: &gt; t.test(x = forbes94$ROS, alternative = &quot;two.sided&quot;, mu = 6, conf.level = 0.90) One Sample t-test data: forbes94$ROS t = 4.1703, df = 795, p-value = 3.377e-05 alternative hypothesis: true mean is not equal to 6 90 percent confidence interval: 6.779904 7.797800 sample estimates: mean of x 7.288852 The results indicate that the 90% confidence interval is \\((6.7799, 7.7978)\\), while the test returns a very small p-value, reported as 3.377e-0552, which allows to largely reject the null hypothesis even using a significance level \\(\\alpha\\) equal to 0.01. 6.2.2 Inference on the proportion of successes in a Bernoulli population In analogy to the case of the mean of a normal population, the function prop.test() provides the similar calculations for the inference on the proportion of successes in a Bernoulli population. The arguments of the function are: x, number of successes observed in the \\(n\\) sample observations n, sample size p, value of the proportion \\(p_0\\) that we want to test under the null hypothesis alternative, the type of test we want to perform, that is, if two-sided (two.sided) or one-sided (less or greater, depending on the direction in \\(H_1\\)) conf.level, the confidence level we want to use correct, with which we can indicate to R whether the so-called Yates correction must be used; this correction has not been presented during the course so we will always set its value to FALSE Also the prop.test() function automatically calculates both the confidence interval and the proportion test. Suppose we want to estimate the proportion of CEO with an MBA in the reference population using the sample available in the data frame forbes94. In addition, we also want to calculate the corresponding 99% confidence interval and test the null hypothesis that the proportion of CEO having an MBA in the population is equal to 30%. The following code allows us to perform these analyses: &gt; prop.test(x = sum(forbes94$MBA == &quot;1&quot;), n = nrow(forbes94), p = 0.3, + alternative = &quot;two.sided&quot;, conf.level = 0.99, correct = FALSE) 1-sample proportions test without continuity correction data: sum(forbes94$MBA == &quot;1&quot;) out of nrow(forbes94), null probability 0.3 X-squared = 5.006, df = 1, p-value = 0.02526 alternative hypothesis: true p is not equal to 0.3 99 percent confidence interval: 0.2256803 0.3057062 sample estimates: p 0.26375 Note that to calculate the number of successes, we used the code sum(forbes94$MBA == “1”), i.e. we first checked with the logical operator == which observations we observed correspond to a “success” (i.e. a value equal to “1”) and we then calculated the sum (sum()) of the resulting logical vector53. The results indicate that the 99% confidence interval is equal to \\((0.2257, 0.3057)\\)54, The p-value of the test, equal to 0.02526, suggests that there is enough empirical evidence coming from the data to reject the null hypothesis \\(H_0: p = 0.30\\) at a significance level \\(\\alpha\\) of 0.05, but not 0.01. 6.2.3 Inference on the comparison between the means of two normal populations The case of the difference between two means is one of those most frequently used in the applications and in the course we have presented some variations, in particular: comparison between the means of two normal populations with known variances, independent samples comparison between the means of two normal populations with unknown but equal variances, independent samples comparison between the means of two normal populations with unknown variances, dependent samples In R there are no tools available for the first case, which we will not present here. To compare the means of two normal populations in R we can still use the t.test() function, but differently from the case of a single mean, we now have to provide the following arguments: x, observed values for the first sample y, observed values for the second sample alternative, the type of test we want to perform, that is, if two-sided (two.sided) or one-sided (less or greater, depending on the direction in \\(H_1\\)) mu, value of the difference in the means we want to test under the null hypothesis paired, which indicates whether the samples are independent (FALSE) or dependent (TRUE) var.equal, which indicates whether the (unknown) variances of the two populations in the case of independent samples are assumed to be equal (TRUE) or not (FALSE) conf.level, confidence level that we want to use 6.2.3.1 Independent Samples Let’s look at an example of a comparison between the means of two normal populations with unknown but equal variances and independent samples. In particular, we compare the mean of the ROS variable in the two groups identified by the MasterPhd variable, which indicates which of the CEOs in 1994 had a Master or PhD degree. We calculate the 95% confidence interval and we carry out the test to verify if the two averages in the population are equal, that is \\[\\begin{equation*} H_0: \\mu_{(\\mbox{MasterPhd}=0)} = \\mu_{(\\mbox{MasterPhd}=1)} \\quad \\mbox{vs.} \\quad H_1: \\mu_{(\\mbox{MasterPhd}=0)} \\ne \\mu_{(\\mbox{MasterPhd}=1)}. \\end{equation*}\\] The following code allows to perform these analyses: &gt; ROS_0 &lt;- forbes94$ROS[forbes94$MasterPhd == &quot;0&quot;] &gt; ROS_1 &lt;- forbes94$ROS[forbes94$MasterPhd == &quot;1&quot;] &gt; t.test(ROS_0, ROS_1, alternative = &quot;two.sided&quot;, mu = 0, paired = FALSE, + var.equal = TRUE, conf.level = 0.95) Two Sample t-test data: ROS_0 and ROS_1 t = -1.7542, df = 794, p-value = 0.07978 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -2.2948277 0.1288676 sample estimates: mean of x mean of y 6.74056 7.82354 The sample mean in the second sample is larger, but the results of the test suggest that there does not seem to be significant differences between the ROS averages in the two populations of companies, those whose CEO does not have a Masters or Phd and those in which the CEO has a Master or a Phd (at least using the common values of 1% and 5% for the significance level). 6.2.3.2 Dependent samples In R we can address the case of dependent samples still using the t.test() function, except that this time the arguments x and y must include the same number observations and the argument paired should be set to TRUE. Let’s see an example using the data contained in the data frame supermarket, already discussed in Section 4.3.2. After loading the data, we create two vectors that respectively contain the data corresponding to the two samples, with the foresight that the first sample will concern the day when the promotion was active and the second one that when it was not. Next, with the t.test() function we calculate the 90% confidence interval for the difference in the means and run the test \\[\\begin{equation*} H_0: \\mu_X - \\mu_Y \\le 0 \\qquad \\mbox{vs.} \\qquad H_1: \\mu_X - \\mu_Y &gt; 0, \\end{equation*}\\] that is, we are interested in checking whether the data suggest that the promotion allowed to increase the average number of visits to the stores55. The following code allows to perform these analyses: &gt; load(&quot;supermarket.RData&quot;) &gt; program_yes &lt;- supermarket$customers[supermarket$program == &quot;yes&quot;] &gt; program_no &lt;- supermarket$customers[supermarket$program == &quot;no&quot;] &gt; t.test(x = program_yes, y = program_no, alternative = &quot;greater&quot;, + paired = TRUE, conf.level = 0.90) Paired t-test data: program_yes and program_no t = 2.0981, df = 9, p-value = 0.03266 alternative hypothesis: true difference in means is greater than 0 90 percent confidence interval: 1.022489 Inf sample estimates: mean of the differences 3 Notice that we chose alternative = “greater” because now we are interested in a one-sided test. The results indicate that, assuming a 5% significance level, the promotion seems to have had a significant effect on the average number of visits since the p-value of the test (0.03266) is smaller than 0.05. 6.2.4 Inference on the linear correlation index In analogy with the other functions presented so far (i.e. t.test() and prop.test()), R provides the function cor.test() for making inference on the linear correlation index \\(\\rho\\) of a bivariate normal population. As for the calculation of the cor() function, cor.test() also requires to specify the two variables whose association we want to evaluate, as well as the arguments alternative and conf.level, that are common to the other functions mentioned above. As an example, let’s use the data frame forbes94 and perform a test to evaluate the absence of linear association between the variables Salary and Age. \\[\\begin{equation*} H_0: \\rho_{(\\mbox{Salary}, \\mbox{Age})} = 0 \\quad \\mbox{vs.} \\quad H_1: \\rho_{(\\mbox{Salary}, \\mbox{Age})} \\ne 0. \\end{equation*}\\] Together with the scatter plot of the two variables, the following code provides the results related to the test: &gt; plot(x = forbes94$Age, y = forbes94$Salary, xlab = &quot;Age&quot;, ylab = &quot;Salary&quot;) Figure 6.14: Scatter plot of the Salary variable versus Age. &gt; cor.test(x = forbes94$Salary, y = forbes94$Age, + alternative = &quot;two.sided&quot;, conf.level = 0.95) Pearson&#39;s product-moment correlation data: forbes94$Salary and forbes94$Age t = 6.7072, df = 787, p-value = 3.788e-11 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.1654221 0.2975008 sample estimates: cor 0.2325333 The sample correlation index between the two variables, equal to 0.2353, indicates a weak linear association, but the low value of the p-value (3.788e-11) shows that this association is highly significant. Recall that this notation represents an alternative way to indicate numbers very close to zero, in this case \\(3.377 \\times 10^{- 5} = 0.00003377\\).↩ Remember that R internally converts the logical value FALSE to 0 and the logical value TRUE to 1.↩ These values do not correspond exactly to the calculation we presented in the course, because R internally uses a slightly different but more precise formula. Remember that the expressions we have seen in this case are approximations based on the normal distribution.↩ In the test, \\(X\\) indicates the population of shops where the promotion is active, while \\(Y\\) denotes the population of shops where the promotion is not active.↩ "]
]
